{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Group 1 - Facebook<span class=\"tocSkip\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the sources we need to scrap was the Facebook website (several pages).This notebook contains the code to retrieve the data and also the Robot to retrieve the data once a week. We scrapped posts on specifics pages. They contain for each publication, one comment, the publication date and number of comment, share and likes.\n",
    "\n",
    "V0 : Soup scrap + click function\n",
    "\n",
    "V2 : Comment recuperation + try of click on (\"more comment\") (only the first post is possible)\n",
    "\n",
    "V4 : Recuperation of likes, comment and share number + description\n",
    "\n",
    "V5 : Code formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import urllib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common import action_chains\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains all the functions we developped during the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicSoup(url, link_driver, n_loop):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            url : url page\n",
    "            link_driver : driver link\n",
    "            n_boucle : scroll time number\n",
    "       out : \n",
    "            bs(driver.page_source, 'html.parser') : driver of the new page\n",
    "            driver : driver\n",
    "    \"\"\"\n",
    "    driver = webdriver.PhantomJS(link_driver)\n",
    "    driver.set_window_size(1400,1000)\n",
    "    driver.get(url)\n",
    "    y = 0\n",
    "        \n",
    "    for _ in range(n_loop):\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(\"+str(y)+\",\"+str(y+2000)+\")\")\n",
    "        time.sleep(2)\n",
    "        y += 2000\n",
    "    return bs(driver.page_source, 'html.parser'), driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clickShow(driver):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            driver : driver\n",
    "\n",
    "    \"\"\"\n",
    "    driver.find_element_by_class_name(\"_3j0u\").click()\n",
    "    elements = driver.find_elements_by_class_name(\"_4ssp\")\n",
    "    action = action_chains.ActionChains(driver)\n",
    "    for el in elements:\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            action.move_to_element(el)\n",
    "            time.sleep(4)\n",
    "            action.click(el)\n",
    "            action.perform()\n",
    "        except:\n",
    "            print(\"err\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_all(text, dic):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            text : character string\n",
    "            dic : dictionary which contains the changes to be made\n",
    "       out : \n",
    "            text : text with all changes made \n",
    "    \"\"\"\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simplification(chain):\n",
    "    # transform M into 1000000,  and K into 1000\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            chain : character string in format \"ex : 300K\"    \n",
    "       out : \n",
    "            int(float(expression) * coef) : integer  \"ex 300 000\"          \n",
    "    \"\"\"\n",
    "    chain = chain.replace(',', '.')\n",
    "    if '.' in chain:\n",
    "        expression = (re.search(\"\\d+\\.\\d+\", chain)).group(0)\n",
    "    else:\n",
    "        expression = (re.search(\"\\d+\", chain)).group(0)\n",
    "    coef = 1\n",
    "    if 'K' in chain:\n",
    "        coef = 1000\n",
    "    if 'M' in chain:\n",
    "        coef = 1000000\n",
    "    return(int(float(expression) * coef))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recovTextBetweenTags(texts: str, separator: str):\n",
    "    \"\"\" Retrieving code between two tags\n",
    "    \n",
    "    Paramters:\n",
    "        texts = Part of soup\n",
    "        separator = Separator of soup\n",
    "    Outers:\n",
    "        description = Text wanted\n",
    "    \"\"\" \n",
    "    text_clean = []\n",
    "    lisI = []\n",
    "    lisS = []\n",
    "\n",
    "    for i in range(0, len(texts)):\n",
    "        if str(texts[i]) == \"<\":\n",
    "            lisI.append(i)\n",
    "        if texts[i] == '>':\n",
    "            lisS.append(i)\n",
    "\n",
    "    len_lis = len(lisI)\n",
    "    for h in range(0, len_lis):\n",
    "        if h < (len_lis-1):\n",
    "            text_clean.append(texts[lisS[h]:lisI[h+1]])\n",
    "\n",
    "    if separator != 'non':\n",
    "        description = str(text_clean).replace('>', '').replace(\n",
    "            ',', '').replace('\\'', '').replace('，', '')\n",
    "        description = description.split(separator)\n",
    "    else:\n",
    "        description = text_clean\n",
    "\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateJs(listURL):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            url1_page\n",
    "            dic \n",
    "       out : \n",
    "            bs(driver.page_source, 'html.parser') : driver of the new page\n",
    "            driver : driver\n",
    "    \"\"\"\n",
    "\n",
    "    name_col = ['Data_Source',\n",
    "                'Airline_Name',\n",
    "                'Airline_Type',\n",
    "                'Region_Operation',\n",
    "                'Aircraft_Type',\n",
    "                'Cabin_Class',\n",
    "                'Type_Of_Lounge',\n",
    "                'Type_Of_Traveller',\n",
    "                'Date_Visit',\n",
    "                'Date_Flown',\n",
    "                'Airport',\n",
    "                'Route',\n",
    "                'Category',\n",
    "                'Category_Detail',\n",
    "                'Cabin_Staff_Service',\n",
    "                'Lounge_Staff_Service',\n",
    "                'Bar_And_Beverages',\n",
    "                'Food_And_Beverages',\n",
    "                'Ground_Service',\n",
    "                'Catering',\n",
    "                'Cleanliness',\n",
    "                'Lounge_Comfort',\n",
    "                'Aisle_Space',\n",
    "                'Wifi_And_Connectivity',\n",
    "                'Inflight_Entertainment',\n",
    "                'Viewing_Tv_Screen',\n",
    "                'Power_Supply',\n",
    "                'Seat',\n",
    "                'Seat_type',\n",
    "                'Seat_Comfort',\n",
    "                'Seat_Legroom',\n",
    "                'Seat_Storage',\n",
    "                'Seat_Width',\n",
    "                'Seat_Recline',\n",
    "                'Washrooms',\n",
    "                'Value_For_Money',\n",
    "                'Overall_Customer_Rating',\n",
    "                'Overall_Service_Rating',\n",
    "                'Overall_Airline_Rating',\n",
    "                'Recommended',\n",
    "                'Date_Review',\n",
    "                'Review',\n",
    "                'Departure_city',\n",
    "                'Arrival_city',\n",
    "                'Nb_bus_taken',\n",
    "                'Nb_train_taken',\n",
    "                'Nb_car_taken',\n",
    "                'Nb_plane_taken',\n",
    "                'Duration',\n",
    "                'Price_min',\n",
    "                'Price_max',\n",
    "                'Title',\n",
    "                'Author',\n",
    "                'Description',\n",
    "                'Date_publication',\n",
    "                'View_Count',\n",
    "                'Likes',\n",
    "                'Dislikes',\n",
    "                'Nb_subscribers',\n",
    "                'Nb_comments',\n",
    "                'Nb_sharing',\n",
    "                'Hashtags',\n",
    "                'Awards',\n",
    "                'Registration',\n",
    "                'Location',\n",
    "                'Contributions_Pers',\n",
    "                'Nb_pertinent_comments_Pers',\n",
    "                'Queuing_Times',\n",
    "                'Terminal_Seating',\n",
    "                'Terminal_Signs',\n",
    "                'Airport_Shopping',\n",
    "                'Experience_At_Airport']\n",
    "\n",
    "    dataAirline = pd.DataFrame(columns=name_col)\n",
    "\n",
    "    lien_driver = \"C:/Users/user/Desktop/phantomjs-2.1.1-windows/bin/phantomjs.exe\"\n",
    "\n",
    "    dic2 = {'.': '', 'avril': '04', 'janvier': '01',\n",
    "            'mars': '03', 'mai': '05', 'juin': '06', 'février': '02', 'juillet': '07', 'décembre': '12',\n",
    "\n",
    "            'août': '08', 'septembre': '09', 'aoÃ»t': '08', 'dÃ©c': '12', 'novembre': '11'}\n",
    "\n",
    "    for ind, url in enumerate(listURL):\n",
    "        ur = url + 'posts/?ref=page_internal'\n",
    "        soup, driver = dynamicSoup(ur, lien_driver, 4)\n",
    "\n",
    "        # get likes\n",
    "        nb_like2 = []\n",
    "        desc = []\n",
    "        share2 = []\n",
    "        date2 = []\n",
    "        nb_com2 = []\n",
    "        comment1 = []\n",
    "        com2 = []\n",
    "        source = []\n",
    "        \n",
    "        for span in soup.findAll('div', attrs={'class': '_5pcr userContentWrapper'}):\n",
    "\n",
    "            nb_like = span.find('span', attrs={'class': '_81hb'})\n",
    "\n",
    "            if nb_like == None:\n",
    "                nb_like2.append(' ')\n",
    "            else:\n",
    "                nb_like2.append(\n",
    "                    int(str((str(Simplification(str(nb_like).replace('\\xa0', '').replace('>', '')))))))\n",
    "\n",
    "            description = span.find(\n",
    "                'div', attrs={'class': '_5pbx userContent _3576'})\n",
    "            if description == None:\n",
    "                desc.append(' ')\n",
    "                source.append('Facebook')\n",
    "            else:\n",
    "                desc.append(translate(str(recovTextBetweenTags(str(description), 'non')).replace(\n",
    "                    '\\'>\\',', '').replace('>', '')))\n",
    "                source.append('Facebook')\n",
    "\n",
    "            date1 = span.find(\n",
    "                'span', attrs={'class': 'timestampContent', 'id': re.compile(r'js_')})\n",
    "            if date1 == None:\n",
    "                date2.append(' ')\n",
    "            else:\n",
    "                date2.append(recovTextBetweenTags(str(date1), 'non'))\n",
    "\n",
    "            share1 = span.find('a', attrs={'class': '_3rwx _42ft'})\n",
    "            if share1 == None:\n",
    "                share2.append(' ')\n",
    "            else:\n",
    "                share2.append(recovTextBetweenTags(str(share1), 'non'))\n",
    "\n",
    "            nb_com = soup.find('a', attrs={'class': '_3hg- _42ft'})\n",
    "            if nb_com == None:\n",
    "                nb_com2.append(' ')\n",
    "            else:\n",
    "                nb_com2.append(recovTextBetweenTags(str(nb_com), 'non'))\n",
    "\n",
    "            com = span.find('span', attrs={'class': '_3l3x'})\n",
    "            if com == None:\n",
    "                com2.append(' ')\n",
    "            else:\n",
    "                com2.append(translate(str(recovTextBetweenTags(str(com), 'non')).replace(\n",
    "                    '>,', '').replace('>', '').replace('\\'\\', ', '')))\n",
    "\n",
    "        for i in range(0, len(date2)):\n",
    "            if('h' in (date2[i])):\n",
    "                date2[i] = str(date.today())\n",
    "            else:\n",
    "                date2[i] = replace_all(str(date2[i]), dic2)\n",
    "                if (re.search(\"^[0-9]*\\s[0-9]*,\", str(date2[i]))) != None:\n",
    "                    date2[i] = date2[i].split(\",\")[0]+' 2020'\n",
    "                    date2[i] = str(datetime.strptime(re.search(\n",
    "                        \"[0-9]* [0-9]* [0-9]*\", date2[i]).text.group(0), '%d %m %Y')).split(\" \")[0]\n",
    "        # get share\n",
    "        for i in range(0, len(share2)):\n",
    "            if share2[i] != ' ':\n",
    "                share2[i] = int(Simplification(str(share2[i]).replace(\n",
    "                    \"partages\", \"\").replace(\"\\xa0\", \"\").replace(\" \", \"\").replace(\">\", \"\")))\n",
    "\n",
    "        # get nb_com\n",
    "        for i in range(0, len(nb_com2)):\n",
    "            nb_com2[i] = Simplification(\n",
    "                str(nb_com2[i]).replace(\" commentaires\", \"\"))\n",
    "\n",
    "        # get comments\n",
    "        comments = driver.find_elements_by_xpath(\n",
    "            \"//div [@data-testid='UFI2Comment/body']\")\n",
    "        for i in range(0, len(comments)):\n",
    "            comment1.append(comments[i].text)\n",
    "        \n",
    "        df_template = pd.DataFrame({'Data_Source': source, 'Likes': nb_like2, 'Description': desc, 'Date_publication': date2,\n",
    "                                    'Nb_sharing': share2, 'Nb_comments': nb_com2, 'Review': com2})\n",
    "        dataAirline = pd.concat([dataAirline, df_template])\n",
    "\n",
    "        print('page numéro ', ind, ' fini')\n",
    "\n",
    "    return(dataAirline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the facebook pages to be scrapped in the pagesFb file, then scrape them one by one.\n",
    "\n",
    "def Create_List_URL(chain):\n",
    "    with open(chain, \"r\") as f :\n",
    "    \n",
    "        fb_pages = f.read()\n",
    "        files = fb_pages.split(\"\\n\")\n",
    "    return(files)\n",
    "\n",
    "def addJSON(file: str, df, creat: bool):\n",
    "    \"\"\"\n",
    "    Add of dataFrame in an existant JSON\n",
    "    Parameters:\n",
    "        file = path of JSON\n",
    "        df = dataFrame of news datas\n",
    "    Outers:\n",
    "        creat = description of fly\n",
    "    \"\"\"\n",
    "    \n",
    "    if creat is False :\n",
    "        with open(file) as train_file:\n",
    "            dict_train = json.load(train_file)\n",
    "        data = pd.read_json(dict_train, orient=\"records\")\n",
    "        df = pd.concat([data, df])\n",
    "    \n",
    "    js = df.to_json(orient='records').replace(\n",
    "        \"[\\\\\\\"[\", '').replace(\"]\\\\\\\"]\", '')\n",
    "    \n",
    "    with open(file, 'w', encoding='utf8') as outfile:\n",
    "        json.dump(js, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate (texte: str) : \n",
    "    \"\"\"\n",
    "    Paramters:\n",
    "        texte = Text to be translated\n",
    "    Outers:\n",
    "        new = Text translated\n",
    "    \"\"\"\n",
    "    try :\n",
    "        new = str(TextBlob(texte).translate(to='en'))\n",
    "        return new\n",
    "    except :\n",
    "        return texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:183: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page numéro  0  fini\n",
      "page numéro  1  fini\n",
      "page numéro  2  fini\n"
     ]
    }
   ],
   "source": [
    "f = Create_List_URL(\"C:/Users/user/Desktop/pagesFb\")\n",
    "r = CreateJs(f[0:3],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics on recovered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/user/Desktop/ScrapFacebook.json') as train_file:\n",
    "    dict_train = json.load(train_file)\n",
    "data = pd.read_json(dict_train, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           ['>17 01, 05:42']\n",
       "1           ['>10 01, 09:51']\n",
       "2            ['>1 01, 05:04']\n",
       "3      ['>26 12 2019, 07:43']\n",
       "4      ['>22 12 2019, 06:40']\n",
       "                ...          \n",
       "390    ['>30 12 2019, 05:08']\n",
       "391    ['>28 12 2019, 02:44']\n",
       "392    ['>26 12 2019, 05:43']\n",
       "393    ['>25 12 2019, 04:35']\n",
       "394    ['>24 12 2019, 04:45']\n",
       "Name: Date_publication, Length: 395, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Date_publication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nb_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>30.493671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>50.895053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>184.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nb_comments\n",
       "count   395.000000\n",
       "mean     30.493671\n",
       "std      50.895053\n",
       "min       1.000000\n",
       "25%       2.000000\n",
       "50%       5.000000\n",
       "75%      34.000000\n",
       "max     184.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Nb_comments']].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
