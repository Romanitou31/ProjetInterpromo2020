{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Group 1 - Data Collection<span class=\"tocSkip\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#V0-:-basic-scrap\" data-toc-modified-id=\"V0-:-basic-scrap-1.0.0.1\"><span class=\"toc-item-num\">1.0.0.1&nbsp;&nbsp;</span>V0 : basic scrap</a></span></li><li><span><a href=\"#V1-:-Feature-adding\" data-toc-modified-id=\"V1-:-Feature-adding-1.0.0.2\"><span class=\"toc-item-num\">1.0.0.2&nbsp;&nbsp;</span>V1 : Feature adding</a></span></li><li><span><a href=\"#V2-:-Generalization\" data-toc-modified-id=\"V2-:-Generalization-1.0.0.3\"><span class=\"toc-item-num\">1.0.0.3&nbsp;&nbsp;</span>V2 : Generalization</a></span></li><li><span><a href=\"#V3-:-Add-Information\" data-toc-modified-id=\"V3-:-Add-Information-1.0.0.4\"><span class=\"toc-item-num\">1.0.0.4&nbsp;&nbsp;</span>V3 : Add Information</a></span></li><li><span><a href=\"#V4-:-Automatization\" data-toc-modified-id=\"V4-:-Automatization-1.0.0.5\"><span class=\"toc-item-num\">1.0.0.5&nbsp;&nbsp;</span>V4 : Automatization</a></span></li><li><span><a href=\"#V5-:-Automatization\" data-toc-modified-id=\"V5-:-Automatization-1.0.0.6\"><span class=\"toc-item-num\">1.0.0.6&nbsp;&nbsp;</span>V5 : Automatization</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Environment\" data-toc-modified-id=\"Environment-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Data-Loading\" data-toc-modified-id=\"Data-Loading-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Data Loading</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Functions</a></span></li></ul></li><li><span><a href=\"#Crawl\" data-toc-modified-id=\"Crawl-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Crawl</a></span></li><li><span><a href=\"#Descriptive-statistics-on-recovered-data\" data-toc-modified-id=\"Descriptive-statistics-on-recovered-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Descriptive statistics on recovered data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the sources we need to scrap was the Airlinequality website. This notebook contains the code to retrieve the data and also the Robot to retrieve the data once a week (when we scraped once a week, we keep only the one week old comments). We scrapped the comments of air travellers. They contain the person's opinion of their flight as well as various notes about the airport and its components. We also collected the date of his flight and his overall rating.\n",
    "\n",
    "#### V0 : basic scrap\n",
    "We only retrieve the title and description of the comment. We also have the notes (not yet processable).\n",
    "\n",
    "#### V1 : Feature adding \n",
    "Starting the treatment. \n",
    "Notable categories can be used (NA not taken into account).\n",
    "Mini-function to switch pages (depending on the total number of comments). Consideration of new categories for the data dictionary.\n",
    "\n",
    "#### V2 : Generalization\n",
    "\n",
    "Bug on number pages corrected.Scrap on all airports.\n",
    "Note \"N/A\" Ok.\n",
    "\n",
    "#### V3 : Add Information\n",
    "\n",
    "Add Date_Review\n",
    "\n",
    "#### V4 : Automatization\n",
    "\n",
    "Begin of respect of quality chart. Scrap over a given number of days + date format changed \n",
    "\n",
    "#### V5 : Automatization\n",
    "\n",
    "Translate of description and title comments. JSON management (we complete it instead of overwriting it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import Request, urlopen\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains all the functions we developped during the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAirportLinks(text_airport: str):\n",
    "    \"\"\" Airport Recovery \n",
    "    Parmeters: \n",
    "        text_airport = description of airport\n",
    "    Outers:\n",
    "        airport = name of airport\n",
    "        link = link of airport\n",
    "    \"\"\"\n",
    "    airport = re.findall(\"\\\">(.*?)</a></li>\", str(text_airport))[0]\n",
    "    link = re.findall(\"href\\=(.*?)>\", str(text_airport))[0].replace(\"\\\"\", \"\")\n",
    "    return airport, link\n",
    "\n",
    "\n",
    "def createDictionnary():\n",
    "    \"\"\"  Creation of the dictionary containing the name of the airport and its URL \n",
    "    Outers: \n",
    "        dic = dictionnary having the name of airport and link\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    root = \"https://www.airlinequality.com\"\n",
    "    url_page = root+\"/review-pages/a-z-airport-reviews/\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "    req = Request(url_page, headers=headers)\n",
    "    webpage = urlopen(req).read()\n",
    "    soup = bs(webpage, 'html.parser')\n",
    "\n",
    "    r = soup.find_all('li')\n",
    "    list_text = [str(val) for val in r if \"href=\\\"/airport-reviews/\" in str(val)\n",
    "                 and \"article\" not in str(val)]\n",
    "    for texte in list_text:\n",
    "        airport, link = getAirportLinks(texte)\n",
    "        dic[airport.rstrip()] = root+link\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texte: str):\n",
    "    \"\"\"\n",
    "    Paramters:\n",
    "        texte = Text to be translated\n",
    "    Outers:\n",
    "        new = Text translated\n",
    "    \"\"\"\n",
    "    try:\n",
    "        new = str(TextBlob(texte).translate(to='en'))\n",
    "        return new\n",
    "    except:\n",
    "        return texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recovTextBetweenTags(texts: str, separator: str):\n",
    "    \"\"\" Retrieving code between two tags\n",
    "\n",
    "    Paramters:\n",
    "        texts = Part of soup\n",
    "        separator = Separator of soup\n",
    "    Outers:\n",
    "        description = Text wanted\n",
    "    \"\"\"\n",
    "    text_clean = []\n",
    "    lisI = []\n",
    "    lisS = []\n",
    "\n",
    "    for i in range(0, len(texts)):\n",
    "        if str(texts[i]) == \"<\":\n",
    "            lisI.append(i)\n",
    "        if texts[i] == '>':\n",
    "            lisS.append(i)\n",
    "\n",
    "    len_lis = len(lisI)\n",
    "    for h in range(0, len_lis):\n",
    "        if h < (len_lis-1):\n",
    "            text_clean.append(texts[lisS[h]:lisI[h+1]])\n",
    "\n",
    "    if separator != 'non':\n",
    "        description = str(text_clean).replace('>', '').replace(\n",
    "            ',', '').replace('\\'', '').replace('ï¼Œ', '')\n",
    "        description = description.split(separator)\n",
    "    else:\n",
    "        description = text_clean\n",
    "\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(dic: dict, nb: int):\n",
    "    \"\"\" Code allows to recup datas. \n",
    "        Calling up all other functions.\n",
    "    Paramters: \n",
    "        dic = dictionnary having name and link airport\n",
    "    Outer:\n",
    "        dataAirline = dataFrame having scrap informations\n",
    "\n",
    "    \"\"\"\n",
    "    name_col = ['Data_Source', 'Airline_Name', 'Airline_Type', 'Region_Operation', 'Aircraft_Type', 'Cabin_Class', 'Type_Of_Lounge',\n",
    "                'Type_Of_Traveller', 'Date_Visit', 'Date_Flown', 'Airport', 'Route', 'Category', 'Category_Detail',\n",
    "                'Cabin_Staff_Service', 'Lounge_Staff_Service', 'Bar_And_Beverages', 'Food_And_Beverages', 'Ground_Service', 'Catering', 'Cleanliness',\n",
    "                'Lounge_Comfort', 'Aisle_Space', 'Wifi_And_Connectivity', 'Inflight_Entertainment', 'Viewing_Tv_Screen', 'Power_Supply',\n",
    "                'Seat', 'Seat_type', 'Seat_Comfort', 'Seat_Legroom', 'Seat_Storage', 'Seat_Width', 'Seat_Recline', 'Washrooms',\n",
    "                'Value_For_Money', 'Overall_Customer_Rating', 'Overall_Service_Rating', 'Overall_Airline_Rating',\n",
    "                'Recommended', 'Departure_city', 'Arrival_city', 'Nb_bus_taken', 'Nb_train_taken',\n",
    "                'Nb_car_taken', 'Nb_plane_taken', 'Duration', 'Price_min', 'Price_max', 'Nb_sharing', 'Awards', 'Registration', 'Language',\n",
    "                'Queuing Times', 'Terminal_Seating', 'Terminal Signs', 'Airport_Shopping', 'Experience_At_Airport', 'Date_Review']\n",
    "\n",
    "    dataAirline = pd.DataFrame(columns=name_col)\n",
    "\n",
    "    for dic_key, dic_val in dic.items():\n",
    "        r = requests.get(dic_val)\n",
    "        page = r.text\n",
    "        soup = bs(page, 'html.parser')\n",
    "        nb_page = Nb_pages(soup)\n",
    "\n",
    "        for j in range(1, nb_page+1):\n",
    "            r = requests.get(dic_val + '/page/' + str(j) + '/')\n",
    "            page = r.text\n",
    "            soup = bs(page, 'html.parser')\n",
    "\n",
    "            Date_Review = dateReview(soup, nb)\n",
    "            title = title_comm(soup, nb)\n",
    "            desc = description(soup, nb)\n",
    "            note = UserNot(soup, nb)\n",
    "            notGlo = NoteGlobal(soup, nb)\n",
    "\n",
    "            airport = []\n",
    "            source = []\n",
    "\n",
    "            for i in range(0, len(desc)):\n",
    "                airport.append(dic_key)\n",
    "                source.append('AirlineQuality')\n",
    "\n",
    "            df = pd.DataFrame(data=[title, desc, note, airport])\n",
    "            df = df.transpose()\n",
    "\n",
    "            Title = df[0]\n",
    "            Review = df[1]\n",
    "            Date_Visit, Terminal_Cleanliness, Food_Beverages, Wifi_Connectivity, Airport_Staff, Recommended, Type_Of_Traveller, Queuing_Times, Terminal_Seating, Terminal_Signs, Airport_Shopping, Experience_At_Airport = transformColInDic(\n",
    "                df[2])\n",
    "            Airport = df[3]\n",
    "\n",
    "            df_template = pd.DataFrame({'Data_Source': source, 'Date_Flown': Date_Visit, 'Cleanliness': Terminal_Cleanliness, 'Food_And_Beverages': Food_Beverages,\n",
    "                                        'Wifi_And_Connectivity': Wifi_Connectivity, 'Cabin_Staff_Service': Airport_Staff, 'Overall_Customer_Rating': notGlo,\n",
    "                                        'Recommended': Recommended, 'Title': Title, 'Review': Review, 'Airport': Airport, 'Type_Of_Traveller': Type_Of_Traveller,\n",
    "                                        'Queuing_Times': Queuing_Times, 'Terminal_Seating': Terminal_Seating, 'Terminal_Signs': Terminal_Signs,\n",
    "                                        'Airport_Shopping': Airport_Shopping, 'Experience_At_Airport': Experience_At_Airport, 'Date_Review': Date_Review})\n",
    "\n",
    "            dataAirline = pd.concat([dataAirline, df_template])\n",
    "\n",
    "    return dataAirline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserNot(soup: str, nb: int):\n",
    "    \"\"\" Function to retrieve users' notes using the two notes functions. \n",
    "        The first one retrieves the maximum notes for each user. \n",
    "        The second one retrieves the category 'NA' \n",
    "\n",
    "    \"\"\"\n",
    "    list_not = notation2(soup, nb)\n",
    "    noteUser = []\n",
    "    value = []\n",
    "    list_total = [' 1', '2', '3', '4', '5']\n",
    "\n",
    "    for z in range(0, len(list_not)):\n",
    "        dico = {}\n",
    "        # Delete of the first element because it's not a user\n",
    "        del list_not[z][0]\n",
    "        # We look if the next note is a value, if yes we take it because when we scrape, we recover the whole list of the barem\n",
    "        # (ex: we note a 4, we recover 12345. Each noted category is separated by non-numeric characters.\n",
    "        for i in range(0, len(list_not[z])-2):\n",
    "            if len(str(list_not[z][i]).replace(' ', '')) > 1:\n",
    "                if len(str(list_not[z][i+1]).replace(' ', '')) > 1:\n",
    "                    if list_not[z][i] not in value:\n",
    "                        dico[list_not[z][i]] = list_not[z][i+1]\n",
    "                        value.append(list_not[z][i+1])\n",
    "                else:\n",
    "                    # The last value of the note list\n",
    "                    j = i\n",
    "                    while str(list_not[z][j+1]) in list_total:\n",
    "                        dico[list_not[z][i]] = list_not[z][j+1]\n",
    "                        j = j + 1\n",
    "        noteUser.append(dico)\n",
    "\n",
    "    counter_user = 0\n",
    "    c_user_not_w_NA = 0\n",
    "    p = notation(soup, nb)\n",
    "    for k in noteUser:\n",
    "        value = []\n",
    "        # Counter of notes for all users\n",
    "        t = 0\n",
    "        for key, val in k.items():\n",
    "            if val != 'N/A':\n",
    "                if val == '5':\n",
    "                    # The category detected by the previous code takes as a note the value scrapped by the function \"notation\"\n",
    "                    noteUser[counter_user][key] = p[c_user_not_w_NA][t]\n",
    "                    t = t + 1\n",
    "                    if t == len(p[c_user_not_w_NA]):\n",
    "                        c_user_not_w_NA = c_user_not_w_NA + 1\n",
    "\n",
    "        counter_user = counter_user + 1\n",
    "    return noteUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a date to standard format\n",
    "def format_date(date):\n",
    "    # Transform a string date into a standard format by trying each\n",
    "    # date format. If you want to add a format, add a try/except in the\n",
    "    # last except\n",
    "    # date : str : the date to transform\n",
    "    # return : m : timedata : format is YYYY-MM-DD HH:MM:SS\n",
    "    date_str = date\n",
    "    #\n",
    "    date_str = date_str.replace(\"st\", \"\").replace(\"th\", \"\").replace(\n",
    "        \"nd\", \"\").replace(\"rd\", \"\").replace(\" Augu \", \" Aug \")\n",
    "    m = None\n",
    "    try:\n",
    "        m = datetime.strptime(date_str, \"%d %B %Y\")\n",
    "    except ValueError:\n",
    "        try:\n",
    "            m = datetime.strptime(date_str, \"%d %b %Y\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                m = datetime.strptime(date_str, \"%Y/%m/%d\")\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    m = datetime.strptime(date_str, \"%d/%m/%Y %H:%M:%S\")\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        m = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            m = datetime.strptime(date_str,\n",
    "                                                  \"%d %m %Y\")\n",
    "                        except ValueError:\n",
    "                            # HERE ADD A FORMAT TO CHECK\n",
    "                            print(\"Format not recognised. \\nConsider \"\n",
    "                                  \"adding a date format \"\n",
    "                                  \"in the function \\\"format_date\\\".\")\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformColInDic(col_dic: dict):\n",
    "    \"\"\" Transforms a dictionary into a set of lists in order to put them in the dataframe.\n",
    "    Parameters:\n",
    "        col_dic = columns\n",
    "    Outers:\n",
    "        Date_Visit = Date of comment\n",
    "        Terminal_Cleanliness = Confort of seat\n",
    "        Food_Beverages = Quality of food and beverages\n",
    "        Wifi_Connectivity = Quality of connection\n",
    "        Airport_Staff = Competence of airport staff\n",
    "        Recommended = Recommandation of user\n",
    "        Type_Of_Traveller = Situation of user (couple, single)\n",
    "        Queuing_Times = Notation of Queuing Times in the airport\n",
    "        Terminal_Seating = Confort of terminal seat \n",
    "        Terminal_Signs = Quality of terminal signs\n",
    "        Airport_shopping = Quality of airport shopping\n",
    "        Experience_At_Airport = Depart or arrival in the airport\n",
    "    \"\"\"\n",
    "    Date_Visit = []\n",
    "    Terminal_Cleanliness = []\n",
    "    Food_Beverages = []\n",
    "    Wifi_Connectivity = []\n",
    "    Airport_Staff = []\n",
    "    Recommended = []\n",
    "    Type_Of_Traveller = []\n",
    "    Queuing_Times = []\n",
    "    Terminal_Seating = []\n",
    "    Airport_shopping = []\n",
    "    Terminal_Signs = []\n",
    "    Experience_At_Airport = []\n",
    "\n",
    "    for i in range(0, len(col_dic)):\n",
    "        if 'Date Visit' in (col_dic[i]).keys():\n",
    "            Date_Visit.append((col_dic[i]['Date Visit']))\n",
    "        else:\n",
    "            Date_Visit.append(' ')\n",
    "\n",
    "        if ' Terminal Cleanliness' in (col_dic[i]).keys():\n",
    "            Terminal_Cleanliness.append((col_dic[i][' Terminal Cleanliness']))\n",
    "        else:\n",
    "            Terminal_Cleanliness.append(' ')\n",
    "\n",
    "        if ' Food Beverages' in (col_dic[i]).keys():\n",
    "            Food_Beverages.append((col_dic[i][' Food Beverages']))\n",
    "        else:\n",
    "            Food_Beverages.append(' ')\n",
    "\n",
    "        if ' Wifi Connectivity' in (col_dic[i]).keys():\n",
    "            Wifi_Connectivity.append((col_dic[i][' Wifi Connectivity']))\n",
    "        else:\n",
    "            Wifi_Connectivity.append(' ')\n",
    "\n",
    "        if ' Airport Staff' in (col_dic[i]).keys():\n",
    "            Airport_Staff.append((col_dic[i][' Airport Staff']))\n",
    "        else:\n",
    "            Airport_Staff.append(' ')\n",
    "\n",
    "        if ' Recommended' in (col_dic[i]).keys():\n",
    "            Recommended.append((col_dic[i][' Recommended']))\n",
    "        else:\n",
    "            Recommended.append(' ')\n",
    "\n",
    "        if 'Type Of Traveller' in (col_dic[i]).keys():\n",
    "            Type_Of_Traveller.append((col_dic[i]['Type Of Traveller']))\n",
    "        else:\n",
    "            Type_Of_Traveller.append(' ')\n",
    "\n",
    "        if 'Queuing Times' in (col_dic[i]).keys():\n",
    "            Queuing_Times.append((col_dic[i]['Queuing Times']))\n",
    "        else:\n",
    "            Queuing_Times.append(' ')\n",
    "\n",
    "        if ' Terminal Seating' in (col_dic[i]).keys():\n",
    "            Terminal_Seating.append((col_dic[i][' Terminal Seating']))\n",
    "        else:\n",
    "            Terminal_Seating.append(' ')\n",
    "\n",
    "        if ' Airport Shopping' in (col_dic[i]).keys():\n",
    "            Airport_shopping.append((col_dic[i][' Airport Shopping']))\n",
    "        else:\n",
    "            Airport_shopping.append(' ')\n",
    "\n",
    "        if ' Terminal Signs' in (col_dic[i]).keys():\n",
    "            Terminal_Signs.append((col_dic[i][' Terminal Signs']))\n",
    "        else:\n",
    "            Terminal_Signs.append(' ')\n",
    "\n",
    "        if 'Experience At Airport' in (col_dic[i]).keys():\n",
    "            Experience_At_Airport.append((col_dic[i]['Experience At Airport']))\n",
    "        else:\n",
    "            Experience_At_Airport.append(' ')\n",
    "\n",
    "    return Date_Visit, Terminal_Cleanliness, Food_Beverages, Wifi_Connectivity, Airport_Staff, Recommended, Type_Of_Traveller, Queuing_Times, Terminal_Seating, Terminal_Signs, Airport_shopping, Experience_At_Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notation(soup: str, nb: int):\n",
    "    \"\"\"\n",
    "    Retrieving notes from the table on the Airline page\n",
    "    Parameters:\n",
    "        soup: soup of the page\n",
    "        nb: number of days for recovery for scraping\n",
    "    Outers:\n",
    "        note = note of users for each category\n",
    "    \"\"\"\n",
    "    note = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            tab_not = span.findAll('span', attrs={'class': 'star fill'})\n",
    "            notation_categ = re.findall(r'[0-9]', str(tab_not))\n",
    "            if len(notation_categ) > 0:\n",
    "                noteUser = []\n",
    "                len_not_categ = len(notation_categ)\n",
    "                for i in range(0, len_not_categ-1):\n",
    "                    if notation_categ[i] >= notation_categ[i+1]:\n",
    "                        noteUser.append(notation_categ[i])\n",
    "\n",
    "                noteUser.append(notation_categ[len_not_categ-1])\n",
    "                note.append(noteUser)\n",
    "\n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_comm(soup: str, nb: int):\n",
    "    \"\"\"\n",
    "    Recovery of title comment\n",
    "    Parameters:\n",
    "        soup = soup of the page\n",
    "        nb = number of days for recovery for scraping\n",
    "    Outers:\n",
    "        title = title comment \n",
    "    \"\"\"\n",
    "    title = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            top = span.findAll('h2', attrs={'class': 'text_header'})\n",
    "            top = translate(recovTextBetweenTags(str(top), 'non'))\n",
    "            title.append(top[0][1:len(top[0])])\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateReview(soup: str, nb: int):\n",
    "    \"\"\"\n",
    "    Recovery of Date comment\n",
    "    Parameters:\n",
    "        soup = soup of the page\n",
    "        nb = number of days for recovery for scraping\n",
    "    Outers:\n",
    "        dateR = date comment \n",
    "    \"\"\"\n",
    "    dateR = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            top = span.findAll('time', attrs={'itemprop': 'datePublished'})\n",
    "            dateR.append(recovTextBetweenTags(str(top), ','))\n",
    "\n",
    "    return dateR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoteGlobal(soup: str, nb: int):\n",
    "    \"\"\"\n",
    "    Recovery of Note Global\n",
    "    Parameters:\n",
    "        soup = soup of the page\n",
    "        nb = number of days for recovery for scraping\n",
    "    Outers:\n",
    "        notGlo = Note Global \n",
    "    \"\"\"\n",
    "    notGlo = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            top = span.findAll('span', attrs={'itemprop': 'ratingValue'})\n",
    "            notGlo.append(recovTextBetweenTags(str(top), ','))\n",
    "\n",
    "    return notGlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description(soup: str, nb: int):\n",
    "    \"\"\"\n",
    "    Recovery of description trip\n",
    "    Parameters:\n",
    "        soup = soup of the page\n",
    "        nb = number of days for recovery for scraping\n",
    "    Outers:\n",
    "        desc = description of fly\n",
    "    \"\"\"\n",
    "    desc = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            top = span.findAll('div', attrs={'class': 'text_content'})\n",
    "            desc.append(translate(recovTextBetweenTags(str(top), ',')))\n",
    "\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notation2(soup: str, nb: int):\n",
    "    note = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "\n",
    "            not_tot_tab = span.findAll(\n",
    "                'table', attrs={'class': 'review-ratings'})\n",
    "            not_tot_tab = (recovTextBetweenTags(str(not_tot_tab), ','))\n",
    "            note.append(str(str(not_tot_tab).replace(\n",
    "                '\\\\n', '').replace('\\\\', '')).split('  '))\n",
    "\n",
    "    Rating = []\n",
    "    for elem in note:\n",
    "        if len(elem) != 0:\n",
    "            Rating.append(elem)\n",
    "\n",
    "    return Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nb_pages(soup: str):\n",
    "    \"\"\"\n",
    "    Calcul of number page of airport comments\n",
    "    Parameters:\n",
    "        soup = soup of the page\n",
    "    Outers:\n",
    "        nb_pages = number page of airport comments\n",
    "    \"\"\"\n",
    "    nb_page_total = soup.find('div', attrs={'class': 'pagination-total'})\n",
    "    if nb_page_total != None:\n",
    "        nb_page_total = str(nb_page_total)\n",
    "        nb_pages = int(nb_page_total[41:len(nb_page_total)-14])//10 + 1\n",
    "    else:\n",
    "        nb_pages = 1\n",
    "    return(nb_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addJSON(file: str, df, creat: bool):\n",
    "    \"\"\"\n",
    "    Add of dataFrame in an existant JSON\n",
    "    Parameters:\n",
    "        file = path of JSON\n",
    "        df = dataFrame of news datas\n",
    "    \"\"\"\n",
    "    if creat is False:\n",
    "        with open(file) as train_file:\n",
    "            dict_train = json.load(train_file)\n",
    "        data = pd.read_json(dict_train, orient=\"records\")\n",
    "        df = pd.concat([data, df])\n",
    "\n",
    "    js = df.to_json(orient='records').replace(\n",
    "        \"[\\\\\\\"[\", '').replace(\"]\\\\\\\"]\", '')\n",
    "\n",
    "    with open(file, 'w', encoding='utf8') as outfile:\n",
    "        json.dump(js, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains the crawl of AirlineQuality website with the execution of all functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of dictionnary having airports names and airports URL\n",
    "dic = createDictionnary()\n",
    "# Starting the scrap function\n",
    "df = scrap(dic, 7)\n",
    "file = '../Results_json/data_Airline.json'\n",
    "# Transformation and Complete of Json\n",
    "addJSON(file, df, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics on recovered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file) as train_file:\n",
    "    dict_train = json.load(train_file)\n",
    "data = pd.read_json(dict_train, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A complet Airline scraping:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "82,5 Mo with\n",
    "38002 rows X 63 columns (18 columns not NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanliness\n",
      "moy 3.139530630905212\n",
      "med 3.0\n",
      "min 1\n",
      "max 5\n",
      "ecart-type 1.375485603444257\n",
      "\n",
      "Food_And_Beverages\n",
      "moy 2.4437046004842613\n",
      "med 2.0\n",
      "min 1\n",
      "max 5\n",
      "ecart-type 1.347665124592081\n",
      "\n",
      "Wifi_And_Connectivity\n",
      "moy 2.5715202967824786\n",
      "med 2.0\n",
      "min 1\n",
      "max 5\n",
      "ecart-type 1.472260229069088\n",
      "\n",
      "Cabin_Staff_Service\n",
      "moy 2.278249406802004\n",
      "med 2.0\n",
      "min 1\n",
      "max 5\n",
      "ecart-type 1.4610990139330031\n",
      "\n",
      "Queuing_Times\n",
      "moy 2.473302891933029\n",
      "med 2.0\n",
      "min 1\n",
      "max 5\n",
      "ecart-type 1.53215247994542\n",
      "\n",
      "Terminal_Seating\n",
      "moy 2.4315887645380734\n",
      "med 2.0\n",
      "min 1\n",
      "max 5\n",
      "ecart-type 1.3953449991820368\n",
      "\n",
      "Terminal_Signs\n",
      "moy 2.78709294269029\n",
      "med 3.0\n",
      "min 1\n",
      "max 5\n",
      "ecart-type 1.4267573846439106\n",
      "\n",
      "Airport_Shopping\n",
      "moy 2.6824801412180053\n",
      "med 3.0\n",
      "min 1\n",
      "max 5\n",
      "ecart-type 1.363486640956753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lis = ['Cleanliness', 'Food_And_Beverages', 'Wifi_And_Connectivity', 'Cabin_Staff_Service',\n",
    "       'Queuing_Times', 'Terminal_Seating', 'Terminal_Signs', 'Airport_Shopping']\n",
    "\n",
    "for j in lis:\n",
    "    cl = []\n",
    "    for i in data[j].transpose():\n",
    "        if i != ' ':\n",
    "            if i != ' N/A':\n",
    "                cl.append(int(i))\n",
    "\n",
    "    print(j)\n",
    "    print('moy', np.mean(cl))\n",
    "    print('med', np.median(cl))\n",
    "    print('min', np.min(cl))\n",
    "    print('max', np.max(cl))\n",
    "    print('ecart-type', np.std(cl))\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
