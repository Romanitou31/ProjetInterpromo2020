{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Group 1 - Data Collection<span class=\"tocSkip\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the sources we need to scrap was the TripAdvisor website. This notebook contains the code to retrieve the data and also the Robot to retrieve informations posted since a date given in the function. \n",
    "This application navigates in the 240 biggest companies of Tripadvisor \n",
    "\n",
    "V0 : Scrap of a single page of reviews\n",
    "\n",
    "V1 : Loops to navigate into several pages of several companies\n",
    "\n",
    "V2 : Add functions to translate Tripadvisor dates into universal type and change chromedriver to phantomJS\n",
    "\n",
    "V3 : Function that retrieves all the reviews that have been posted in the last n days (n is chosen by user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from datetime import datetime, timedelta\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains all the functions we developped during the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language and translate it in english\n",
    "def translate (text) : \n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            text : character string \n",
    "       out : \n",
    "            text : text translated in english\n",
    "    \"\"\"\n",
    "    try :\n",
    "        new = str(TextBlob(text).translate(to='en'))\n",
    "        return new\n",
    "    except :\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words with a dictionnary\n",
    "def replace_all(text, dic):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            text : character string\n",
    "            dic : dictionary which contains the changes to be made\n",
    "       out : \n",
    "            text : text with all changes made \n",
    "    \"\"\"\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing TripAdvisor commentaries dates type into universal one\n",
    "def Date_Convertion(date,dico_date):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            date : character string\n",
    "            dico_date : dictionnary to translate Tripadvisor's dates\n",
    "       out : \n",
    "            date : date day/month/year \n",
    "    \"\"\"    \n",
    "    date = date.lower()    \n",
    "    if \"hier\" in date:\n",
    "        date = (datetime.today() - timedelta(days=1))\n",
    "    elif \"aujourd'hui\" in date:\n",
    "        date = datetime.today()\n",
    "    else:\n",
    "        date = replace_all(str(date),dico_date)\n",
    "        if (re.search(\"\\d\\d\\d\\d\", date)) == None:\n",
    "            date = datetime.strptime(date + \" \" + datetime.today().strftime('%Y'), '%d %b %Y' )\n",
    "        else:\n",
    "            date = datetime.strptime(date, '%b %Y')\n",
    "    return date.strftime('%d %b %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds elements by x_path and testing if the function returns something or not\n",
    "def Find_With_XPath(objet,el):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            object : \"WebElement\" html \n",
    "            dic : dictionary which contains the changes to be made\n",
    "       out : \n",
    "            text : text with all changes made \n",
    "    \"\"\"\n",
    "    ret = objet.find_elements_by_xpath(el)\n",
    "    if ret: ret = ret[0].text\n",
    "    else : ret = np.nan\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds a date in character string format and converts it in a date format\n",
    "def format_date(date):\n",
    "    \"\"\"Documentation\n",
    "       Parameters :\n",
    "             date : String format\n",
    "       out :\n",
    "             m : timedata : format is YYYY-MM-DD HH:MM:SS\n",
    "    \"\"\"\n",
    "    date_str = date\n",
    "    #\n",
    "    date_str = date_str.replace(\"st\",\"\").replace(\"th\",\"\")\\\n",
    "        .replace(\"nd\",\"\").replace(\"rd\",\"\").replace(\" Augu \",\" Aug \")\n",
    "    m = None\n",
    "    try:\n",
    "        m = datetime.strptime(date_str, \"%d %B %Y\")\n",
    "    except ValueError:\n",
    "        try:\n",
    "            m = datetime.strptime(date_str, \"%d %b %Y\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                m = datetime.strptime(date_str, \"%Y/%m/%d\")\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    m = datetime.strptime(date_str,\"%d/%m/%Y %H:%M:%S\")\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        m = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    except ValueError:\n",
    "                        try :\n",
    "                            m = datetime.strptime(date_str,\n",
    "                                                       \"%d %m %Y\")\n",
    "                        except ValueError:\n",
    "                            # HERE ADD A FORMAT TO CHECK\n",
    "                            print(\"Format not recognised. \\nConsider \"\n",
    "                                  \"adding a date format \"\n",
    "                                  \"in the function \\\"format_date\\\".\")\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big function scraping a list of reviews from different pages from a list of the 240 biggest airline companies\n",
    "# of Tripadvisor\n",
    "def Lets_Scrape(n_days,phantom_js_file,df_companies_file,export_file):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            n_days : int : Number of days before today where you scrap reviews\n",
    "            phantom_js_file : character string : way to acceed and execute phantomJs\n",
    "            df_companies_file : character string : way to acceed and import the df_companies file\n",
    "            export_file : character string : file where to export final json data\n",
    "    \"\"\"   \n",
    "    ##### Settings\n",
    "    # Set the driver and the size of the virtual window\n",
    "    driver = webdriver.PhantomJS(phantom_js_file)\n",
    "    driver.set_window_size(2560,1600)\n",
    "    \n",
    "    # Links' companies import\n",
    "    df_compagnies = pd.read_csv(df_companies_file, sep='§', engine='python', index_col=0, encoding=\"utf-8\")\n",
    "    \n",
    "    # Informations we will get on TripAdvisor\n",
    "    df_comp_reviews = pd.DataFrame(columns=[\"location\",\"tags\",\"date_voyage\",\"commentaire\",\"date_commentaire\",\n",
    "                                        \"Espace pour les jambes\",\"Confort du siège\",\n",
    "                                        \"Divertissement à bord\",\"Service client\",\"Rapport qualité/prix\",\"Propreté\",\n",
    "                                        \"Enregistrement et embarquement\",\"Restauration et boissons\"])\n",
    "    \n",
    "    # Dictionnary to universalise months\n",
    "    dico_date = {'.':'','avr':'apr','janv':'jan','mars':'mar','mai':'may','juin':'jun','févr':'feb','juil':'jul','déc':'dec','août':'aug','sept':'sep'}\n",
    "\n",
    "    # Companies' links loop\n",
    "    #for company in range(len(df_compagnies)):\n",
    "    for company in range(113,116):\n",
    "        # Select name and link of the current company into comp_name and comp_url\n",
    "        comp_name, comp_url = df_compagnies.loc[company,:]\n",
    "        print(comp_name)\n",
    "        comp_name, comp_url\n",
    "        driver.get(comp_url)   \n",
    "    \n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Click on the button \"all languages\"\n",
    "        btn_langages = driver.find_elements_by_xpath(\"//div[contains(@class,'is-3-tablet')]//ul[@class='location-review-review-list-parts-ReviewFilter__filter_table--1H9KD']//li//label[@for='LanguageFilter_0']\")    \n",
    "        btn_langages[0].find_elements_by_xpath(\".//span\")[0].click()\n",
    "        \n",
    "        # Waiting 5 seconds for the new all languages commentaries to load\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Select number of total commentaries' number     \n",
    "        nb_pages = driver.find_elements_by_xpath(\"//a[@class='pageNum ']\")\n",
    "        if nb_pages == []:\n",
    "            nb_pages = 1\n",
    "        else:\n",
    "            nb_pages = int(nb_pages[-1].text)\n",
    "        print(nb_pages)\n",
    "        # Loop on every pages\n",
    "        for page in range(nb_pages):\n",
    "\n",
    "            # Start chrono\n",
    "            start = time.time()\n",
    "\n",
    "            # Make a random waiting time to simulate human behavior\n",
    "            wait_time_long = 0.5 + np.random.random(1)[0]/10\n",
    "            wait_time_small = 0.5 + np.random.random(1)[0]/10\n",
    "\n",
    "            # Wait for loading to finish\n",
    "            print(\"Récupération de la page en cours\")\n",
    "            time.sleep(wait_time_long)\n",
    "\n",
    "            # Go to the bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "            time.sleep(wait_time_small)\n",
    "\n",
    "            # Load comments details\n",
    "            butons = driver.find_elements_by_xpath(\"//*[contains(@class,'ui_icon caret-down location-review-review-list-parts-ExpandableReview')]\")\n",
    "            if butons : \n",
    "                if butons[0].is_displayed() :\n",
    "                    butons[0].click()\n",
    "                    time.sleep(wait_time_small)\n",
    "\n",
    "            # Select every block of comment and count them\n",
    "            results = driver.find_elements_by_xpath(\"//div[@data-tab='TABS_REVIEWS'][1]//div[@class=''][1]//*[contains(@class, 'location-review-card-Card__ui_card')]\")\n",
    "\n",
    "            # Loop on every block of comment\n",
    "            for block in results:\n",
    "\n",
    "                # block 1\n",
    "                block_user_info = block.find_elements_by_xpath(\"div[contains(@class,'social-member-event-MemberEventOnObjectBlock')]\")[0]\n",
    "\n",
    "                # Select the date review\n",
    "                date_review = block_user_info.find_elements_by_xpath(\"div[1]/div[2]/span\")\n",
    "                date_review = date_review[0].get_attribute('innerHTML').split('</a>')[-1].split(' a écrit un avis le ')[-1]\n",
    "\n",
    "                # Testing if the date review is in the period \n",
    "                pivot_date = format_date(Date_Convertion(date_review,dico_date)) >= (datetime.now() - timedelta(n_days))\n",
    "\n",
    "                if pivot_date:\n",
    "                    # Select user name\n",
    "                    user_name = block_user_info.find_elements_by_xpath(\"div[1]/div[2]/span/a\")\n",
    "                    if user_name : user_name = user_name[0].get_attribute('href').split('/')[-1]\n",
    "                    else : user_name = np.nan\n",
    "\n",
    "                    # Select location where the user posted the comment\n",
    "                    user_location = Find_With_XPath(block_user_info,\"div[1]/div[3]/span/span[contains(@class,'MemberHometown__hometown')]\")\n",
    "\n",
    "                    # Select the user statistics\n",
    "                    user_stats = block_user_info.find_elements_by_xpath(\"div[1]/div[3]/span[contains(@class,'MemberHeaderStats__stat_item')]/span\")\n",
    "                    dict_stats = {}\n",
    "                    for us in user_stats:\n",
    "                        stat_name = us.get_attribute('innerHTML').split('</span>')[-1].strip()\n",
    "                        stat_value = us.find_elements_by_xpath(\"span\")[0].text\n",
    "                        dict_stats[stat_name] = stat_value\n",
    "\n",
    "                    # block 2\n",
    "                    block_user_review = block.find_elements_by_xpath(\"div[contains(@class,'location-review-review-list-parts-SingleReview__mainCol')]\")[0]\n",
    "\n",
    "                    # Select hashtags\n",
    "                    tags = block_user_review.find_elements_by_xpath(\"div[contains(@class,'location-review-review-list-parts-RatingLine')]\")\n",
    "                    if tags : tags = tags[0].text.split(\"\\n\")\n",
    "                    else : tags = np.nan\n",
    "\n",
    "                    # Select title\n",
    "                    title = translate(Find_With_XPath(block_user_review,\"div[@data-test-target='review-title']\"))\n",
    "\n",
    "                    # Select the date flown\n",
    "                    date_flown = block_user_review.find_elements_by_xpath(\".//span[contains(@class,'location-review-review-list-parts-EventDate__event_date')]\")\n",
    "                    if date_flown : date_flown = date_flown[0].get_attribute('innerHTML').split(\"</span>\")[-1].strip()\n",
    "                    else : date_flown = np.nan\n",
    "\n",
    "                    # Select the commentary\n",
    "                    commentary = translate(Find_With_XPath(block_user_review,\".//q[contains(@class,'location-review-review-list-parts-ExpandableReview__reviewText')]\"))\n",
    "\n",
    "                    # Select the global notes\n",
    "                    vol_note = block_user_review.find_elements_by_xpath(\".//div[contains(@class,'location-review-review-list-parts-RatingLine')]/span\")\n",
    "                    if vol_note : vol_note = vol_note[0].get_attribute('class').split('_')[-1]\n",
    "                    else : vol_note = np.nan\n",
    "\n",
    "                    # Select the specific note(s), user can post from 0 to 8 of these notes\n",
    "                    notes = block_user_review.find_elements_by_xpath(\".//div[contains(@class,'location-review-review-list-parts-AdditionalRatings__ratings')]/div\")\n",
    "                    dict_notes = {}\n",
    "                    for e in notes:\n",
    "                        categorie_name = e.find_elements_by_xpath(\"span\")[1].text\n",
    "                        categorie_note = e.find_elements_by_xpath(\"span/span\")[0].get_attribute('class').split('_')[-1][0]\n",
    "                        dict_notes[categorie_name] = categorie_note\n",
    "                    dict_notes['Overall_Customer_Rating'] = int(vol_note[0])*2\n",
    "\n",
    "                    # make a local dictionnaries wich will be put in the final dataframe\n",
    "                    all_values = {\"location\" : user_location,\n",
    "                                  \"tags\" : tags,\n",
    "                                  \"date_voyage\" : date_flown,\n",
    "                                  \"commentaire\" : commentary,\n",
    "                                  \"date_commentaire\" : date_review,\n",
    "                                  \"Airline_Name\" : comp_name,\n",
    "                                  \"Data_Source\" : 'TripAdvisor',\n",
    "                                  \"Title\" : title}\n",
    "                    all_values.update(dict_stats)\n",
    "                    all_values.update(dict_notes)\n",
    "                    df_comp_reviews = df_comp_reviews.append(pd.Series(all_values), ignore_index=True, sort=False)\n",
    "            # Print to control the advencement        \n",
    "            print('Page numéro ', page,' récupérée de la compagnie ',comp_name)\n",
    "            print(\"Temps d'exécution : \", time.time()-start)\n",
    "            print('---------------------------------------------------') \n",
    "\n",
    "            # Click on the button to acceed the next page\n",
    "            if pivot_date:\n",
    "                next_page = driver.find_elements_by_xpath(\"//*//a[@class='ui_button nav next primary ']\")\n",
    "                if next_page : \n",
    "                    next_page = next_page[0].click()\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # Merge of 4 columns into 2            \n",
    "    if 'contribution' in df_comp_reviews.columns:\n",
    "        for i in range(len(df_comp_reviews)):\n",
    "            if str(df_comp_reviews['contribution'][i])!=\"nan\":\n",
    "                df_comp_reviews['contributions'][i]=float(1)\n",
    "        del df_comp_reviews['contribution'] \n",
    "        \n",
    "    if 'vote utile' in df_comp_reviews.columns:\n",
    "        for i in range(len(df_comp_reviews)):\n",
    "            if str(df_comp_reviews['vote utile'][i])!=\"nan\":\n",
    "                df_comp_reviews['votes utiles'][i]=float(1)                \n",
    "        del df_comp_reviews['vote utile'] \n",
    "    #Re-nomation of the variables to match with the chart\n",
    "    print(df_comp_reviews.shape)\n",
    "    print(df_comp_reviews.head())\n",
    "    df_comp_reviews.columns=[\"Location\",\"Hashtags\",\"Date_Flown\",\"Review\",\"Date_Review\",\"Seat_Legroom\",\"Seat_Comfort\",\n",
    "                             \"Inflight_Entertainment\",\"Overall_Service_Rating\",\"Value_For_Money\",\"Cleanliness\",\n",
    "                             \"Registration\",\"Food_And_Beverages\",\"Airline_Name\",\"Data_Source\",\n",
    "                             \"Overall_Customer_Rating\",\"Title\",\"Contributions_Pers\",\"Nb_Pertinent_Comments\"]                \n",
    "    \n",
    "    name_col = pd.DataFrame(['Data_Source', 'Airline_Name', 'Airline_Type', 'Region_Operation', 'Aircraft_Type', 'Cabin_Class', 'Type_Of_Lounge',\n",
    "                'Type_Of_Traveller', 'Date_Visit', 'Date_Flown', 'Airport', 'Route', 'Category', 'Category_Detail',\n",
    "                'Cabin_Staff_Service', 'Lounge_Staff_Service', 'Bar_And_Beverages', 'Food_And_Beverages', 'Ground_Service', 'Catering', 'Cleanliness',\n",
    "                'Lounge_Comfort', 'Aisle_Space', 'Wifi_And_Connectivity', 'Inflight_Entertainment', 'Viewing_Tv_Screen', 'Power_Supply',\n",
    "                'Seat', 'Seat_type', 'Seat_Comfort', 'Seat_Legroom', 'Seat_Storage', 'Seat_Width', 'Seat_Recline', 'Washrooms',\n",
    "                'Value_For_Money', 'Overall_Customer_Rating', 'Overall_Service_Rating', 'Overall_Airline_Rating',\n",
    "                'Recommended', 'Departure_city', 'Arrival_city', 'Nb_bus_taken', 'Nb_train_taken',\n",
    "                'Nb_car_taken', 'Nb_plane_taken', 'Duration', 'Price_min', 'Price_max', 'Nb_sharing', 'Awards', 'Registration', 'Language',\n",
    "                'Queuing Times', 'Terminal_Seating', 'Terminal Signs', 'Airport_Shopping', 'Experience_At_Airport', 'Date_Review'])\n",
    "    \n",
    "    #Puting the Tripadvisor informations into the general dataframe\n",
    "    df_comp_reviews = pd.concat([df_comp_reviews, name_col])\n",
    "    \n",
    "    # Export the data in Json\n",
    "    c = df_comp_reviews.to_json(orient='records')\n",
    "    with open(export_file, 'w', encoding='utf8') as outfile:\n",
    "            json.dump(c, outfile, ensure_ascii=False,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains the crawl of Tripadvisor website with the execution of the scraping function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lion Air\n",
      "408\n",
      "Récupération de la page en cours\n",
      "Page numéro  0  récupérée de la compagnie  Lion Air\n",
      "Temps d'exécution :  2.930304527282715\n",
      "---------------------------------------------------\n",
      "Binter\n",
      "12\n",
      "Récupération de la page en cours\n",
      "Page numéro  0  récupérée de la compagnie  Binter\n",
      "Temps d'exécution :  2.8710250854492188\n",
      "---------------------------------------------------\n",
      "Sky Airline\n",
      "1\n",
      "Récupération de la page en cours\n",
      "Page numéro  0  récupérée de la compagnie  Sky Airline\n",
      "Temps d'exécution :  2.987510919570923\n",
      "---------------------------------------------------\n",
      "(0, 13)\n",
      "Empty DataFrame\n",
      "Columns: [location, tags, date_voyage, commentaire, date_commentaire, Espace pour les jambes, Confort du siège, Divertissement à bord, Service client, Rapport qualité/prix, Propreté, Enregistrement et embarquement, Restauration et boissons]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 13 elements, new values have 19 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-d6ded7f324bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLets_Scrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'phantomjs.exe'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'df_compagnies.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'test.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-607a3ca447c7>\u001b[0m in \u001b[0;36mLets_Scrape\u001b[1;34m(n_days, phantom_js_file, df_companies_file, export_file)\u001b[0m\n\u001b[0;32m    183\u001b[0m                              \u001b[1;34m\"Inflight_Entertainment\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Overall_Service_Rating\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Value_For_Money\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Cleanliness\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                              \u001b[1;34m\"Registration\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Food_And_Beverages\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Airline_Name\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Data_Source\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                              \"Overall_Customer_Rating\",\"Title\",\"Contributions_Pers\",\"Nb_Pertinent_Comments\"]                \n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     name_col = pd.DataFrame(['Data_Source', 'Airline_Name', 'Airline_Type', 'Region_Operation', 'Aircraft_Type', 'Cabin_Class', 'Type_Of_Lounge',\n",
      "\u001b[1;32m~\\Anaconda31\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5191\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5192\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5193\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5194\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda31\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda31\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    181\u001b[0m             raise ValueError(\n\u001b[0;32m    182\u001b[0m                 \u001b[1;34m\"Length mismatch: Expected axis has {old} elements, new \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                 \u001b[1;34m\"values have {new} elements\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mold_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m             )\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 13 elements, new values have 19 elements"
     ]
    }
   ],
   "source": [
    "Lets_Scrape(2,'phantomjs.exe','df_compagnies.csv','test.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics on recovered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SortiePourStatsDesc.json') as train_file:\n",
    "            dict_train = json.load(train_file)\n",
    "df = pd.read_json(dict_train, orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_describe=[\"Overall_Customer_Rating\",\"Cleanliness\",\"Food_And_Beverages\",\"Inflight_Entertainment\",\"Registration\",\"Seat_Comfort\",\"Seat_Legroom\",\"Value_For_Money\"]\n",
    "df[col_to_describe].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize = (10, 15))\n",
    "for i in range(8):\n",
    "    axes=fig.add_subplot(8,1,i+1)\n",
    "    plt.hist(df[col_to_describe[i]])\n",
    "    axes.set_xlabel(col_to_describe[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
