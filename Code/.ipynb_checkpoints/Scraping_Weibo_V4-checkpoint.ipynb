{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Group 1 - Data Collection<span class=\"tocSkip\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the sources we need to scrap was the Weibo a chinese website. This notebook contains the code to retrieve the data, translate chinese to english and also the Robot to retrieve the data once a week.\n",
    "\n",
    "V0 : simple scraping and tags finding\n",
    "\n",
    "V1 : transform code into functions and getting dates of publication \n",
    "\n",
    "V2 : adding traduction\n",
    "\n",
    "V3 : fixing bugs\n",
    "\n",
    "V4: improving quality and fixing last bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from textblob import TextBlob\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains all the functions we developped during the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToDate(text: str):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            text : character string     \n",
    "       out : \n",
    "            date of  the publication'    \n",
    "    \"\"\"\n",
    "\n",
    "    if \"今天\" in text:\n",
    "        return dt.date.today()\n",
    "    numbers_list = re.search(\"([0-9]*\\w)*\", text).group(0)\n",
    "    numbers_list = re.findall(r\"([0-9]+)\", numbers_list)\n",
    "    if len(numbers_list) == 3:\n",
    "        year = numbers_list[0]\n",
    "        month = numbers_list[1]\n",
    "        day = numbers_list[2]\n",
    "\n",
    "    elif len(numbers_list) == 2:\n",
    "        now = dt.date.today()\n",
    "        year = now.year\n",
    "        month = numbers_list[0]\n",
    "        day = numbers_list[1]\n",
    "    else:\n",
    "        return dt.date.today()\n",
    "    return dt.date(int(year), int(month), int(day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextBetweenTags(texts: str, separator: str):\n",
    "    \"\"\" Retrieving code between two tags\n",
    "\n",
    "    Paramters:\n",
    "        texts = Part of soup\n",
    "        separator = Separator of soup\n",
    "    Outers:\n",
    "        description = Text wanted\n",
    "    \"\"\"\n",
    "    text_clean = []\n",
    "    lisI = []\n",
    "    lisS = []\n",
    "\n",
    "    for i in range(0, len(texts)):\n",
    "        if str(texts[i]) == \"<\":\n",
    "            lisI.append(i)\n",
    "        if texts[i] == '>':\n",
    "            lisS.append(i)\n",
    "\n",
    "        len_lis = len(lisI)\n",
    "    for h in range(0, len_lis):\n",
    "        if h < (len_lis-1):\n",
    "            text_clean.append(texts[lisS[h]:lisI[h+1]])\n",
    "\n",
    "    if separator != 'non':\n",
    "        description = str(text_clean).replace('>', '').replace(\n",
    "            ',', '').replace('\\'', '').replace('，', '')\n",
    "        description = description.split(separator)\n",
    "    else:\n",
    "        description = text_clean\n",
    "\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createJson(publication_date, description: str, shares: str, comm: str, like: str):\n",
    "    \"\"\"Documentation\n",
    "\n",
    "       creation of the json file named weibo.json \n",
    "\n",
    "       Parameters:\n",
    "            publication_date : Date of the publication\n",
    "            description : publication description\n",
    "            shares : number or shares\n",
    "            comm : comment\n",
    "            like : number of likes\n",
    "\n",
    "    \"\"\"\n",
    "    nom_col = ['Data_Source', 'Airline_Name', 'Airline_Type', 'Region_Operation', 'Aircraft_Type', 'Cabin_Class', 'Type_Of_Lounge',\n",
    "               'Type_Of_Traveller', 'Date_Visit', 'Date_Flown', 'Airport', 'Route', 'Category', 'Category_Detail',\n",
    "               'Cabin_Staff_Service', 'Lounge_Staff_Service', 'Bar_And_Beverages', 'Food_And_Beverages', 'Ground_Service',\n",
    "               'Catering', 'Cleanliness', 'Lounge_Comfort', 'Aisle_Space', 'Wifi_And_Connectivity', 'Inflight_Entertainment',\n",
    "               'Viewing_Tv_Screen', 'Power_Supply', 'Date_publication'\n",
    "               'Seat', 'Seat_type', 'Seat_Comfort', 'Seat_Legroom', 'Seat_Storage', 'Seat_Width', 'Seat_Recline', 'Washrooms',\n",
    "               'Value_For_Money', 'Overall_Customer_Rating', 'Overall_Service_Rating', 'Overall_Airline_Rating',\n",
    "               'Recommended', 'Departure_city', 'Arrival_city', 'Nb_bus_taken', 'Nb_train_taken',\n",
    "               'Nb_car_taken', 'Nb_plane_taken', 'Duration', 'Price_min', 'Price_max', 'Nb_sharing', 'Awards',\n",
    "               'Registration']\n",
    "\n",
    "    df = pd.DataFrame(columns=nom_col)\n",
    "\n",
    "    detailVideos = {}\n",
    "\n",
    "    for i in nom_col:\n",
    "        detailVideos[i] = ' '\n",
    "\n",
    "    if len(shares.replace(' ', '')) == 0:\n",
    "        shares = '0'\n",
    "\n",
    "    detailVideos['Description'] = translate(description)\n",
    "    detailVideos['Nb_sharing'] = shares\n",
    "    detailVideos['Likes'] = like\n",
    "    detailVideos['Date_publication'] = str(publication_date)\n",
    "    detailVideos['Nb_comments'] = comm\n",
    "\n",
    "    with open('weibo.json', 'a', encoding='utf8') as outfile:\n",
    "        json.dump(detailVideos, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            text : character string     \n",
    "       out : \n",
    "            text : text translated in english     \n",
    "    \"\"\"\n",
    "    try:\n",
    "        new = str(TextBlob(text).translate(to='en'))\n",
    "        return new\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            text : character string\n",
    "            dic  : dictionnary structured as (textToReplace : newText)\n",
    "       out : \n",
    "            text : original text with string replacement as defined in the dictionnary  \n",
    "    \"\"\"\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of equation\n",
    "\n",
    "AirCompanies = [\"American Airlines\", \"Air Canada\", \"Air France\", \"Air Algerie\", \"Air India\", \"Aerolineas Argentinas\", \"Royal Air Maroc\", \"Finnair\", \"Alitalia \", \" Nouvelair\", \"Air China\", \"Cathay Pacific\", \"Delta Airlines\", \"Aer Lingus\", \"Emirates\", \"Ethiopian Airlines\", \"Icelandair\", \"Hawaiian Airlines\", \"Iberia\", \"Meridiana\", \"Japan Airlines\", \"KLM\", \"Air Malta\", \"Lan Airlines\", \"Luxair\", \"LIAT\", \"LOT Polish Airlines\", \"Air Madagascar\", \"Air Mauritius\", \"Austrian Airlines\", \"Qatar Airways\", \"South African Airways\", \"SAS Scandinavian Airlines\", \"Brussels Airlines\",\n",
    "                \"Singapore Airlines\", \"Corsair\", \"Aeroflot\", \"Thai Airways\", \"Turkish Airlines\", \"TAP Portugal\", \"Air Transat\", \"Tunisair\", \"Air Caraibes\", \"United Airlines\", \"Air Austral\", \"Air Europa\", \"Easyjet\", \"Vietnam Airlines\", \"Virgin Atlantic\", \"Air Corsica\", \"Condor\", \"Flybe\", \"Aegean Airlines\", \"Air Tahiti Nui\", \"Aigle Azur\", \"HOP!\", \"Jet Airways\", \"Etihad Airways\", \"Etihad Airways\", \"Oman Air\", \"XL Airways\", \"Ryanair LTD\", \"Vueling \", \"Norwegian\", \"Transavia France\", \"Germanwings\", \"TUI Fly Belgium\", \"Air Arabia\", \"WOW air\", \"Wizz Air\", \"Air Asia\", \"Volotea\", \"southwest airlines\"]\n",
    "BoeingModel = [\"Boeing 717\", \"Boeing 727\", \"Boeing 737-200\", \"Boeing 737-300\", \"Boeing 737-400\", \"Boeing 737-500\", \"Boeing 737-600\", \"Boeing 737-700\", \"Boeing 737-700ER\", \"Boeing 737-800\", \"Boeing 737-900\", \"Boeing 737-900ER\", \"Boeing 737 MAX 7\",\n",
    "               \"Boeing 737 MAX 8\", \"Boeing 737 MAX 9\", \"Boeing 737 MAX 10\", \"Boeing 747-200\", \"Boeing 747-400\", \"Boeing 757-200\", \"Boeing 757-300\", \"Boeing 767-200\", \"Boeing 767-300\", \"Boeing 767-300ER\", \"Boeing 767-400ER\", \"Boeing 777 Triple Seven\", \"Boeing 787 DreamLiner\"]\n",
    "AirbusModel = [\"A300\", \"A300-600ST\", \"A318\", \"A319\", \"A320-100\", \"A320-200\", \"A320neo\", \"A321-100\", \"A321-200\", \"A330-200\",\n",
    "               \"A330-300\", \"A330-200F\", \"A330-500\", \"A340-200\", \"A340-300\", \"A340-500\", \"A340-600\", \"A350-900\", \"A350-1000\", \"A380-800\", \"A220-300\"]\n",
    "motsCles = [\"trip\", \"fly\", \"plane\", \"airplane\", \"flight\"]\n",
    "\n",
    "url_list = []\n",
    "for companie in AirCompanies:\n",
    "    for model in AirbusModel:\n",
    "        url_list.append('https://s.weibo.com/weibo/'+companie+'%2520'+model)\n",
    "    for model in BoeingModel:\n",
    "        url_list.append('https://s.weibo.com/weibo/'+companie+'%2520'+model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s.weibo.com/weibo/American Airlines%2520A300  complete\n",
      "https://s.weibo.com/weibo/American Airlines%2520A300-600ST  complete\n",
      "https://s.weibo.com/weibo/American Airlines%2520A318  complete\n",
      "https://s.weibo.com/weibo/American Airlines%2520A319  complete\n",
      "https://s.weibo.com/weibo/American Airlines%2520A320-100  complete\n",
      "Every url completed\n"
     ]
    }
   ],
   "source": [
    "# nb days to filter:\n",
    "nb_day = 7\n",
    "# in exemple : nb_day = 7 gives you only post that have been written this week\n",
    "\n",
    "with open('weibo.json', 'w', encoding='utf8') as outfile:\n",
    "    json\n",
    "\n",
    "\n",
    "for url in url_list:\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    web_page = urlopen(req).read()\n",
    "    soup = BeautifulSoup(web_page, 'html.parser')\n",
    "    counter = 0\n",
    "\n",
    "    #     description\n",
    "\n",
    "    texte = str(soup.findAll('p', attrs={'node-type': 'feed_list_content'}))\n",
    "    description = getTextBetweenTags(texte, '\\\\n')\n",
    "    del description[0]\n",
    "\n",
    "    # like / share, comment / description\n",
    "    like_list = []\n",
    "    share_list = []\n",
    "    comment_list = []\n",
    "    for span in soup.findAll('a', attrs={'title': \"赞\"}):\n",
    "        text = span.text.strip()\n",
    "        if len(text.replace(' ', '')) == 0:\n",
    "            text = '0'\n",
    "        like_list.append(text)\n",
    "    share = []\n",
    "    comment = []\n",
    "    for span in soup.findAll('a'):\n",
    "\n",
    "        text = span.text.strip()\n",
    "\n",
    "        # get number of comments\n",
    "        if text.startswith('转发'):\n",
    "            if len(text[3:]) == 0:\n",
    "                comment_list.append('0')\n",
    "            else:\n",
    "                comment_list.append(text[3:])\n",
    "\n",
    "        # get number of share\n",
    "        if text.startswith('评论'):\n",
    "            if len(text[3:]) == 0:\n",
    "                share_list.append('0')\n",
    "            else:\n",
    "                share_list.append(text[3:])\n",
    "    # add publication stat in json if publication is \"new\"\n",
    "    for span1 in soup.findAll('p', attrs={'class': 'from'}):\n",
    "        for span in span1.findAll('a', attrs={'target': '_blank'}):\n",
    "            if textToDate(span.text.strip()) >= (dt.date.today() - dt.timedelta(days=(nb_day))):\n",
    "                (createJson(textToDate(span.text.strip(\n",
    "                )), description[counter], share_list[counter], comment_list\n",
    "                    [counter], like_list[counter]))\n",
    "            counter += 1\n",
    "    print(url, ' complete')\n",
    "print('Every url completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
