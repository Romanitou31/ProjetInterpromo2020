{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import json\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupTexteEntreBalise(texte, separateur):\n",
    "    \n",
    "    texte2 = []\n",
    "    lisI = []\n",
    "    lisS = []\n",
    "    \n",
    "    for i in range(0,len(texte)):\n",
    "        if str(texte[i]) == \"<\":\n",
    "            lisI.append(i)\n",
    "        if texte[i] == '>':\n",
    "            lisS.append(i)   \n",
    "\n",
    "    taille = len(lisI)\n",
    "    for h in range(0,taille):\n",
    "        if h < (taille-1):\n",
    "            texte2.append(texte[lisS[h]:lisI[h+1]])\n",
    "    \n",
    "    if separateur != 'non':\n",
    "        description = str(texte2).replace('>','').replace(',','').replace('\\'','').replace('，','')\n",
    "        description = description.split(separateur)\n",
    "    else:\n",
    "        description = texte2\n",
    "    \n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supprimeBlanc(Author):\n",
    "    Author2 = []\n",
    "    for elem in Author:\n",
    "        if len(elem) != 0:\n",
    "            Author2.append(elem)\n",
    "    return Author2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupWeibo(listURL):\n",
    "    \n",
    "    df = pd.DataFrame(columns = [0, 1, 2, 3, 4])\n",
    "    for i in listURL:\n",
    "        \n",
    "        r = requests.get(i)\n",
    "        page = r.text\n",
    "        soup = bs(page,'html.parser')\n",
    "        \n",
    "        # Description Post\n",
    "        texte = soup.findAll('p',attrs={'node-type':'feed_list_content'})\n",
    "        texte = str(texte)\n",
    "        description = recupTexteEntreBalise(texte, '\\\\n')\n",
    "        del description[0]\n",
    "        \n",
    "        description = np.array(description).T\n",
    " \n",
    "        # Date Post\n",
    "        date = soup.findAll('p',attrs={'class':'from'})\n",
    "        date = str(date)\n",
    "        date2 = recupTexteEntreBalise(texte, 'non')\n",
    "        \n",
    "        date2 = np.array(date2).T\n",
    "        date2 = re.findall(r'[0-9]+年[0-9]+月[0-9]+日',str(date2))\n",
    "\n",
    "        # Auteur Post\n",
    "        auteur = soup.findAll('a',attrs={'class':'name'})\n",
    "        auteur = str(auteur)\n",
    "        Author = recupTexteEntreBalise(auteur, ' ')\n",
    "        \n",
    "        Author2 = supprimeBlanc(Author)\n",
    "        \n",
    "        \n",
    "        # Partage / Commentaire, Avis / Like\n",
    "        aime = soup.findAll('div',attrs={'class':\"card-act\"})\n",
    "        aime = str(aime)\n",
    "        aime2 = recupTexteEntreBalise(aime, 'non')\n",
    "        \n",
    "        Partage = []\n",
    "        Comm = []\n",
    "        Like = []\n",
    "\n",
    "        for i in range(0,len(aime2)):\n",
    "            if aime2[i].startswith('> 转发'):\n",
    "                Partage.append(str(re.findall(r'[0-9]+',str(aime2[i]))).replace('[','').replace(']','').replace('\\'',''))\n",
    "            if aime2[i].startswith('>评论'):\n",
    "                Comm.append(str(re.findall(r'[0-9]+',str(aime2[i]))).replace('[','').replace(']','').replace('\\'',''))\n",
    "            test = re.findall(r'>[0-9]+',aime2[i])\n",
    "            if (str(test))!='[]':\n",
    "                test = re.findall(r'[0-9]+',str(test))\n",
    "                Like.append(str(test).replace('[','').replace(']','').replace('\\'','').replace('>',''))\n",
    "            else:\n",
    "                if aime2[i-7].startswith('>评论'):\n",
    "                    Like.append(str(aime2[i]).replace('[','').replace(']','').replace('\\'','').replace('>',''))\n",
    "\n",
    "        Comm = np.array(Comm).T\n",
    "        Partage = np.array(Partage).T\n",
    "        Like = np.array(Like).T\n",
    "        \n",
    "        s = pd.DataFrame(data=(description,Author2,Partage,Comm,Like))\n",
    "        detailVideos = s.transpose()\n",
    "\n",
    "        df = pd.concat([df, detailVideos])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation de recherche\n",
    "liste1 = ['airbus', 'boeing']\n",
    "liste2 = ['A320', 'A360']\n",
    "a = []\n",
    "for i in liste1:\n",
    "    for j in liste2:\n",
    "        a.append('https://s.weibo.com/weibo/'+i+'%2520'+j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = recupWeibo(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://s.weibo.com/weibo/Airbus%2520A380')\n",
    "page = r.text\n",
    "soup = bs(page,'html.parser')\n",
    "\n",
    "date = soup.findAll('p',attrs={'class':'from'})\n",
    "date = str(date)\n",
    "description = recupTexteEntreBalise(date, 'non')\n",
    "description = np.array(description).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019年11月23日',\n",
       " '2019年05月29日',\n",
       " '2019年04月09日',\n",
       " '2019年12月31日',\n",
       " '2019年12月28日',\n",
       " '2019年12月25日',\n",
       " '2019年12月21日',\n",
       " '2019年12月21日',\n",
       " '2019年12月19日',\n",
       " '2019年12月18日',\n",
       " '2019年12月16日',\n",
       " '2019年12月14日',\n",
       " '2019年12月13日',\n",
       " '2019年11月21日',\n",
       " '2019年11月04日',\n",
       " '2019年10月13日',\n",
       " '2019年09月28日',\n",
       " '2019年09月24日',\n",
       " '2019年09月18日',\n",
       " '2019年09月14日',\n",
       " '2019年09月06日',\n",
       " '2019年09月01日']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[0-9]+年[0-9]+月[0-9]+日',str(description))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
