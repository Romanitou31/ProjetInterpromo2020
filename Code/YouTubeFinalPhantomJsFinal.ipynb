{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created on 09/01/2020\n",
    "# Group1\n",
    "# @authors: benjamin anton\n",
    "\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta,date\n",
    "import time\n",
    "import requests\n",
    "from urllib.request import Request, urlopen\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path\n",
    "path = '/home/sid2019-13/Téléchargements/chromedriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate (texte) : \n",
    "    try :\n",
    "        new = str(TextBlob(texte).translate(to='en'))\n",
    "        return new\n",
    "    except :\n",
    "        return texte\n",
    "    \n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVideos(soup, limitDate=None):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            soup : soup for the request \n",
    "            limitDate : date on which we will filter video\n",
    "       out : \n",
    "            list_Videos : search equation videos list        \n",
    "    \"\"\"\n",
    "    videos = soup.findAll('a', attrs={'class': 'yt-uix-tile-link'})\n",
    "    dates = soup.findAll('ul', attrs={'class': 'yt-lockup-meta-info'})\n",
    "    \n",
    "    list_Videos = []\n",
    "    for i in range(len(videos)):\n",
    "        date = DateCalculation(str(dates[i]).split(\n",
    "            \"</li><li>\")[0].split(\"<li>\")[-1])\n",
    "        if limitDate is None:\n",
    "            list_Videos.append('https://www.youtube.com'+videos[i]['href'])\n",
    "        elif date > limitDate:\n",
    "            list_Videos.append('https://www.youtube.com'+videos[i]['href'])\n",
    "            \n",
    "    return list_Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create equation research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airline_Companies = [\n",
    "    \"American Airlines\",\n",
    "    \"Air Canada\",\n",
    "    \"Air France\",\n",
    "    \"Air Algerie\",\n",
    "    \"Air India\",\n",
    "    \"Aerolineas Argentinas\",\n",
    "    \"Royal Air Maroc\",\n",
    "    \"Finnair\",\n",
    "    \"Alitalia \",\n",
    "    \" Nouvelair\",\n",
    "    \"Air China\",\n",
    "    \"Cathay Pacific\",\n",
    "    \"Delta Airlines\",\n",
    "    \"Aer Lingus\",\n",
    "    \"Emirates\",\n",
    "    \"Ethiopian Airlines\",\n",
    "    \"Icelandair\",\n",
    "    \"Hawaiian Airlines\",\n",
    "    \"Iberia\",\n",
    "    \"Meridiana\",\n",
    "    \"Japan Airlines\",\n",
    "    \"KLM\",\n",
    "    \"Air Malta\",\n",
    "    \"Lan Airlines\",\n",
    "    \"Luxair\",\n",
    "    \"LIAT\",\n",
    "    \"LOT Polish Airlines\",\n",
    "    \"Air Madagascar\",\n",
    "    \"Air Mauritius\",\n",
    "    \"Austrian Airlines\",\n",
    "    \"Qatar Airways\",\n",
    "    \"South African Airways\",\n",
    "    \"SAS Scandinavian Airlines\",\n",
    "    \"Brussels Airlines\",\n",
    "    \"Singapore Airlines\",\n",
    "    \"Corsair\",\n",
    "    \"Aeroflot\",\n",
    "    \"Thai Airways\",\n",
    "    \"Turkish Airlines\",\n",
    "    \"TAP Portugal\",\n",
    "    \"Air Transat\",\n",
    "    \"Tunisair\",\n",
    "    \"Air Caraibes\",\n",
    "    \"United Airlines\",\n",
    "    \"Air Austral\",\n",
    "    \"Air Europa\",\n",
    "    \"Easyjet\",\n",
    "    \"Vietnam Airlines\",\n",
    "    \"Virgin Atlantic\",\n",
    "    \"Air Corsica\",\n",
    "    \"Condor\",\n",
    "    \"Flybe\",\n",
    "    \"Aegean Airlines\",\n",
    "    \"Air Tahiti Nui\",\n",
    "    \"Aigle Azur\",\n",
    "    \"HOP!\",\n",
    "    \"Jet Airways\",\n",
    "    \"Etihad Airways\",\n",
    "    \"Etihad Airways\",\n",
    "    \"Oman Air\",\n",
    "    \"XL Airways\",\n",
    "    \"Ryanair LTD\",\n",
    "    \"Vueling \",\n",
    "    \"Norwegian\",\n",
    "    \"Transavia France\",\n",
    "    \"Germanwings\",\n",
    "    \"TUI Fly Belgium\",\n",
    "    \"Air Arabia\",\n",
    "    \"WOW air\",\n",
    "    \"Wizz Air\",\n",
    "    \"Air Asia\",\n",
    "    \"Volotea\",\n",
    "    \"southwest airlines\"\n",
    "]\n",
    "Airline_Companies = [compagnies.replace(\n",
    "    ' ', '+') for compagnies in Airline_Companies]\n",
    "\n",
    "\n",
    "Boeing_Models = [\n",
    "    \"Boeing 717\",\n",
    "    \"Boeing 727\",\n",
    "    \"Boeing 737-200\",\n",
    "    \"Boeing 737-300\",\n",
    "    \"Boeing 737-400\",\n",
    "    \"Boeing 737-500\",\n",
    "    \"Boeing 737-600\",\n",
    "    \"Boeing 737-700\",\n",
    "    \"Boeing 737-700ER\",\n",
    "    \"Boeing 737-800\",\n",
    "    \"Boeing 737-900\",\n",
    "    \"Boeing 737-900ER\",\n",
    "    \"Boeing 737 MAX 7\",\n",
    "    \"Boeing 737 MAX 8\",\n",
    "    \"Boeing 737 MAX 9\",\n",
    "    \"Boeing 737 MAX 10\",\n",
    "    \"Boeing 747-200\",\n",
    "    \"Boeing 747-400\",\n",
    "    \"Boeing 757-200\",\n",
    "    \"Boeing 757-300\",\n",
    "    \"Boeing 767-200\",\n",
    "    \"Boeing 767-300\",\n",
    "    \"Boeing 767-300ER\",\n",
    "    \"Boeing 767-400ER\",\n",
    "    \"Boeing 777 Triple Seven\",\n",
    "    \"Boeing 787 DreamLiner\"\n",
    "]\n",
    "Boeing_Models = [models.replace(' ', '+') for models in Boeing_Models]\n",
    "\n",
    "\n",
    "Airbus_Models = [\n",
    "    \"A300\",\n",
    "    \"A300-600ST\",\n",
    "    \"A318\",\n",
    "    \"A319\",\n",
    "    \"A320-100\",\n",
    "    \"A320-200\",\n",
    "    \"A320neo\",\n",
    "    \"A321-100\",\n",
    "    \"A321-200\",\n",
    "    \"A330-200\",\n",
    "    \"A330-300\",\n",
    "    \"A330-200F\",\n",
    "    \"A330-500\",\n",
    "    \"A340-200\",\n",
    "    \"A340-300\",\n",
    "    \"A340-500\",\n",
    "    \"A340-600\",\n",
    "    \"A350-900\",\n",
    "    \"A350-1000\",\n",
    "    \"A380-800\",\n",
    "    \"A220-300\"\n",
    "]\n",
    "\n",
    "key_words = [\n",
    "    \"trip\",\n",
    "    \"fly\",\n",
    "    \"plane\",\n",
    "    \"airplane\",\n",
    "    \"flight\"\n",
    "]\n",
    "\n",
    "\n",
    "equations = []\n",
    "for comp in Airline_Companies:\n",
    "    for mod in Airbus_Models:\n",
    "        equations.append(comp+\"+\"+mod)\n",
    "    for mod in Boeing_Models:\n",
    "        equations.append(comp+\"+\"+mod)\n",
    "\n",
    "# quibbling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChangeDate(chain):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            chain : character string in format \"ex :il ya 300 ans\"    \n",
    "       out : \n",
    "            expression.group(0) : character string in format  \"ex 300 ans\"          \n",
    "    \"\"\"\n",
    "    expression = re.search(\"[0-9]*\\s[a-zA-Z]*$\", chain)\n",
    "    return(expression.group(0))\n",
    "\n",
    "\n",
    "def Simplification(chain):\n",
    "    # transform M into 1000000,  and K into 1000\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            chain : character string in format \"ex : 300K\"    \n",
    "       out : \n",
    "            int(float(expression) * coef) : integer  \"ex 300 000\"          \n",
    "    \"\"\"\n",
    "    chain = chain.replace(',', '.')\n",
    "    if '.' in chain:\n",
    "        expression = (re.search(\"\\d+\\.\\d+\", chain)).group(0)\n",
    "    else:\n",
    "        expression = (re.search(\"\\d+\", chain)).group(0)\n",
    "    coef = 1\n",
    "    if 'k' in chain:\n",
    "        coef = 1000\n",
    "    if 'M' in chain:\n",
    "        coef = 1000000\n",
    "    return(int(float(expression) * coef))\n",
    "\n",
    "\n",
    "def DateCalculation(chain):\n",
    "    # calculate the exact date of publication of the video\n",
    "    \"\"\"Documentation    \n",
    "       Parameters :\n",
    "            chain : character string in format \"ex : il ya 300 ans\"    \n",
    "       out : \n",
    "            chain : date           \n",
    "    \"\"\"\n",
    "\n",
    "    chain = (chain).replace('il y a ', '')\n",
    "    day_nb=0\n",
    "    if 'hier' in chain:\n",
    "        day_nb = 1\n",
    "    else:\n",
    "        day_nb = int((re.search(\"\\d+\", chain)).group(0))\n",
    "    if 'mois' in chain:\n",
    "        day_nb = day_nb * 30\n",
    "    if 'an' in chain:\n",
    "        day_nb = day_nb * 365\n",
    "    if 'semaine' in chain:\n",
    "        day_nb = day_nb * 7\n",
    "\n",
    "    now = date.today()\n",
    "    return str(now - timedelta(days=(day_nb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the url list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def URLlist(Research_Equations,limitDate):\n",
    "    # returns the list of videos of the search equation passed in parameter\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            Research_Equations : search equation we want to launch    \n",
    "       out : \n",
    "            list_Videos : search equation videos list        \n",
    "    \"\"\"\n",
    "    root_URL = \"https://www.youtube.com/results?search_query=\"\n",
    "    #ResearchEquations = \"airbus+A380\"\n",
    "\n",
    "    r = requests.get(root_URL + Research_Equations)\n",
    "    page = r.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    list_Videos = getVideos(soup,limitDate)\n",
    "    return list_Videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function that creates our filled Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCodeHTML(URL_list, fig):\n",
    "    \"\"\"Documentation    \n",
    "\n",
    "       Parameters:\n",
    "            URL_list : list of url   \n",
    "            fig : url index processed\n",
    "       out :  \n",
    "            BeautifulSoup(web_page, 'html.parser') : return html code of the url page      \n",
    "    \"\"\"\n",
    "    url = URL_list[fig]\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    web_page = urlopen(req).read()\n",
    "    return BeautifulSoup(web_page, 'html.parser')\n",
    "\n",
    "\n",
    "def CreateJs(comment, nb_com, soup, comment_date):\n",
    "    # create the json file with all the information on the page\n",
    "    \"\"\"Documentation    \n",
    "\n",
    "       Parameters:\n",
    "            comment : character string of the comment\n",
    "            nb_com : number of comment\n",
    "            comment_date : comment date\n",
    "\n",
    "    \"\"\"\n",
    "    name_col = ['Airline_Name',\n",
    "                'Airline_Type',\n",
    "                'Region_Operation',\n",
    "                'Aircraft_Type',\n",
    "                'Cabin_Class',\n",
    "                'Type_Of_Lounge',\n",
    "                'Type_Of_Traveller',\n",
    "                'Date_Visit',\n",
    "                'Date_Flown',\n",
    "                'Airport',\n",
    "                'Route',\n",
    "                'Category',\n",
    "                'Category_Detail',\n",
    "                'Cabin_Staff_Service',\n",
    "                'Lounge_Staff_Service',\n",
    "                'Bar_And_Beverages',\n",
    "                'Food_And_Beverages',\n",
    "                'Ground_Service',\n",
    "                'Catering', 'Cleanliness',\n",
    "                'Lounge_Comfort',\n",
    "                'Aisle_Space',\n",
    "                'Wifi_And_Connectivity',\n",
    "                'Inflight_Entertainment',\n",
    "                'Viewing_Tv_Screen',\n",
    "                'Power_Supply',\n",
    "                'Seat',\n",
    "                'Seat_type',\n",
    "                'Seat_Comfort',\n",
    "                'Seat_Legroom',\n",
    "                'Seat_Storage',\n",
    "                'Seat_Width',\n",
    "                'Seat_Recline',\n",
    "                'Washrooms',\n",
    "                'Value_For_Money',\n",
    "                'Overall_Customer_Rating',\n",
    "                'Overall_Service_Rating',\n",
    "                'Overall_Airline_Rating',\n",
    "                'Recommended',\n",
    "                'Departure_city',\n",
    "                'Arrival_city',\n",
    "                'Nb_bus_taken',\n",
    "                'Nb_train_taken',\n",
    "                'Nb_car_taken',\n",
    "                'Nb_plane_taken',\n",
    "                'Duration',\n",
    "                'Price_min',\n",
    "                'Price_max',\n",
    "                'Nb_sharing',\n",
    "                'Awards',\n",
    "                'Registration',\n",
    "                'Language']\n",
    "\n",
    "    soup = soup\n",
    "    video_details = {}\n",
    "\n",
    "# Fill data\n",
    "\n",
    "    video_details['Data_Source'] = 'Youtube'\n",
    "\n",
    "    for i in range(39):\n",
    "        video_details[name_col[i]] = ' '\n",
    "\n",
    "    video_details['Date_Review'] = DateCalculation(comment_date)\n",
    "    video_details['Review'] = translate(comment)\n",
    "\n",
    "    for i in range(39, 48):\n",
    "        video_details[name_col[i]] = ' '\n",
    "\n",
    "# get the title of the video\n",
    "    video_details['Title'] = soup.find(\n",
    "        'span', attrs={'class': 'watch-title'}).text.strip()\n",
    "\n",
    "# get the name of the chain\n",
    "    for script in soup.findAll('script', attrs={'type': 'application/ld+json'}):\n",
    "        channelDescription = json.loads(script.text.strip())\n",
    "        video_details['Author'] = channelDescription['itemListElement'][0]['item']['name']\n",
    "\n",
    "# get description\n",
    "    video_details['Description'] = soup.find(\n",
    "        'p', attrs={'id': \"eow-description\"}).text.strip()\n",
    "\n",
    "# get the date of publication\n",
    "    dic = {'.':'','avr':'apr','janv':'jan','mars':'mar','mai':'may','juin':'jun','févr':'feb','juil':'jul','déc':'dec','août':'aug','sept':'sep','aoÃ»t':'aug','dÃ©c':'dec'}\n",
    "    var_date_of_public = soup.find('strong',attrs={'class': \"watch-time-text\"}).text.strip().replace('.','')\n",
    "    var_date_of_public = replace_all(var_date_of_public,dic)\n",
    "    video_details['Date_publication'] = str(datetime.strptime(re.search(\"[0-9][0-9]* [a-zA-Z]* [0-9]*\", var_date_of_public).group(0), '%d %b %Y')). replace('00:00:00', '')\n",
    "\n",
    "# get the number of views\n",
    "    video_details['View_Count'] = (soup.find(\n",
    "        'div', attrs={'class': 'watch-view-count'}).text.strip()).replace('vues', '')\n",
    "\n",
    "# get a likes button\n",
    "    for span in soup.findAll('', attrs={'class': \"yt-uix-button yt-uix-button-size-default yt-uix-button-opacity yt-uix-button-has-icon no-icon-markup like-button-renderer-like-button like-button-renderer-like-button-unclicked yt-uix-clickcard-target yt-uix-tooltip\"}):\n",
    "        video_details['Likes'] = span.find(\n",
    "            'span', attrs={'class': 'yt-uix-button-content'}).text.strip()\n",
    "\n",
    "# get a dislikes button\n",
    "    for button in soup.findAll('button', attrs={'class': \"yt-uix-button yt-uix-button-size-default yt-uix-button-opacity yt-uix-button-has-icon no-icon-markup like-button-renderer-dislike-button like-button-renderer-dislike-button-unclicked yt-uix-clickcard-target yt-uix-tooltip\"}):\n",
    "        video_details['Dislikes'] = button.find(\n",
    "            'span', attrs={'class': 'yt-uix-button-content'}).text.strip()\n",
    "\n",
    "# get subscriber number\n",
    "    if (soup.find('span', attrs={'class': 'yt-subscription-button-subscriber-count-branded-horizontal yt-subscriber-count'}) == None):\n",
    "        video_details[\"Nb_subscribers\"] = 0\n",
    "    else:\n",
    "        video_details[\"Nb_subscribers\"] = Simplification(soup.find('span', attrs={\n",
    "                                                         'class': 'yt-subscription-button-subscriber-count-branded-horizontal yt-subscriber-count'}).text.strip())\n",
    "\n",
    "    video_details['Nb_comments'] = (nb_com).replace('commentaires', '')\n",
    "\n",
    "    video_details[name_col[48]] = ' '\n",
    "# get hashtags\n",
    "    hashtags = []\n",
    "\n",
    "    for span in soup.findAll('span', attrs={'class': 'standalone-collection-badge-renderer-text'}):\n",
    "        for a in span.findAll('a', attrs={'class': 'yt-uix-sessionlink'}):\n",
    "            hashtags.append(a.text.strip())\n",
    "    video_details['hashtags'] = hashtags\n",
    "\n",
    "    for i in range(49, 51):\n",
    "        video_details[name_col[i]] = ' '\n",
    "\n",
    "    video_details['Language'] = 'unknown'\n",
    "\n",
    "    if re.search(\"([a-z]).*\", str(comment).lower()) : \n",
    "        try :\n",
    "            video_details['Language'] = detect(comment)\n",
    "        except :\n",
    "            video_details['Language'] = 'unknown'\n",
    "\n",
    "        \n",
    "    with open('data.json', 'a', encoding='utf8') as outfile:\n",
    "        json.dump(video_details, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def scroll(url, nb_scroll):\n",
    "    #scroll down in the page\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            url : page url \n",
    "            nb_scroll : number of times you scroll  \n",
    "       out : \n",
    "            BeautifulSoup(driver.page_source, 'html.parser') : the new page after scroll\n",
    "            \n",
    "\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    driver =  webdriver.PhantomJS('/home/sid2019-13/Téléchargements/phantomjs-2.1.1-linux-x86_64/bin/phantomjs')\n",
    "    driver.get(url)\n",
    "    Y = 0\n",
    "    for _ in range(nb_scroll):\n",
    "        time.sleep(4)\n",
    "        driver.execute_script(\"window.scrollTo(\"+str(Y)+\",\"+str(Y+800)+\")\")\n",
    "        Y += 1200\n",
    "\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vidéo numéro :  0  fini\n",
      "vidéo numéro :  1  fini\n",
      "vidéo numéro :  2  fini\n",
      "vidéo numéro :  3  fini\n",
      "vidéo numéro :  4  fini\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-465-c4bbfa866206>\u001b[0m in \u001b[0;36mscroll\u001b[0;34m(url, nb_scroll)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_scroll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"window.scrollTo(\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a new json\n",
    "\n",
    "with open('data.json', 'w', encoding='utf8') as outfile:\n",
    "    json\n",
    "\n",
    "for Equation in equations[25:35]:\n",
    "    list_Videos = URLlist(Equation,None)\n",
    "    # len(list_Videos)\n",
    "    if len(list_Videos) > 1 :\n",
    "        for URL_unique in range(min(len(list_Videos),7)):\n",
    "            soup1 = scroll(list_Videos[URL_unique], 4)\n",
    "            SoupCréeJS = GetCodeHTML(list_Videos, URL_unique)\n",
    "        # date comment\n",
    "            date1 = []\n",
    "            for span1 in soup1.findAll('span', attrs={'class': \"comment-renderer-time\"}):\n",
    "                a = (span1.find('a', attrs={'class': \"yt-uix-sessionlink spf-link\"}).text.strip())\n",
    "                date1.append(a)\n",
    "            date_Track = 0\n",
    "            for span in soup1.findAll('div', attrs={'class': 'comment-renderer-text-content'}):\n",
    "                if span.text.strip() != '':\n",
    "                    comment = span.text.strip()\n",
    "                    nb_com = re.search(\"[0-9][0-9]*\", (soup.find('h2', \n",
    "                                                                 attrs={'class': 'comment-section-header-renderer'}).text.strip()).replace('\\xa0',' ').replace('\\u202f','')).group(0)\n",
    "                    CreateJs(comment, nb_com, SoupCréeJS, date1[date_Track])\n",
    "                    date_Track += 1\n",
    "            print('vidéo numéro : ', URL_unique, ' fini')\n",
    "        print('équation finie :', Equation)\n",
    "print('extraction complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-12-23 '"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {'.':'','avr':'apr','janv':'jan','mars':'mar','mai':'may','juin':'jun','févr':'feb','juil':'jul','déc':'dec','août':'aug','sept':'sep','aoÃ»t':'aug','dÃ©c':'dec'}\n",
    "var_date_of_public = SoupCréeJS.find('strong',attrs={'class': \"watch-time-text\"}).text.strip().replace('.','')\n",
    "var_date_of_public = replace_all(var_date_of_public,dic)\n",
    "var_date_of_public\n",
    "str(datetime.strptime(re.search(\"[0-9][0-9]* [a-zA-Z]* [0-9]*\", var_date_of_public).group(0), '%d %b %Y')). replace('00:00:00', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-457-300d19369430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSoupCréeJS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'watch-title'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "SoupCréeJS.find('span', attrs={'class': 'watch-title'}).text.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
