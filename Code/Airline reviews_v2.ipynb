{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import Request, urlopen\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getAirportLinks(text_airport: str):\n",
    "    \"\"\" Airport Recovery \n",
    "    Parmeters: \n",
    "        text_airport = description of airport\n",
    "    Outers:\n",
    "        airport = name of airport\n",
    "        link = link of airport\n",
    "    \"\"\" \n",
    "    airport = re.findall(\"\\\">(.*?)</a></li>\", str(text_airport))[0]\n",
    "    link = re.findall(\"href\\=(.*?)>\", str(text_airport))[0].replace(\"\\\"\", \"\")\n",
    "    return airport, link\n",
    "\n",
    "\n",
    "def createDictionnary():\n",
    "    \"\"\"  Creation of the dictionary containing the name of the airport and its URL \n",
    "    Outers: \n",
    "        dic = dictionnary having the name of airport and link\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    root = \"https://www.airlinequality.com\"\n",
    "    url_page = root+\"/review-pages/a-z-airport-reviews/\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "    req = Request(url_page, headers=headers)\n",
    "    webpage = urlopen(req).read()\n",
    "    soup = bs(webpage, 'html.parser')\n",
    "\n",
    "    r = soup.find_all('li')\n",
    "    list_text = [str(val) for val in r if \"href=\\\"/airport-reviews/\" in str(val)\n",
    "                 and \"article\" not in str(val)]\n",
    "    for texte in list_text:\n",
    "        airport, link = getAirportLinks(texte)\n",
    "        dic[airport.rstrip()] = root+link\n",
    "\n",
    "    return dic\n",
    "\n",
    "\n",
    "dic = createDictionnary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate (texte) : \n",
    "    try :\n",
    "        new = str(TextBlob(texte).translate(to='en'))\n",
    "        return new\n",
    "    except :\n",
    "        return texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recovTextBetweenTags(texts: str, separator: str):\n",
    "    \"\"\" Retrieving code between two tags\n",
    "    \n",
    "    Paramters:\n",
    "        texts = Part of soup\n",
    "        separator = Separator of soup\n",
    "    Outers:\n",
    "        description = Text wanted\n",
    "    \"\"\" \n",
    "    text_clean = []\n",
    "    lisI = []\n",
    "    lisS = []\n",
    "\n",
    "    for i in range(0, len(texts)):\n",
    "        if str(texts[i]) == \"<\":\n",
    "            lisI.append(i)\n",
    "        if texts[i] == '>':\n",
    "            lisS.append(i)\n",
    "\n",
    "    len_lis = len(lisI)\n",
    "    for h in range(0, len_lis):\n",
    "        if h < (len_lis-1):\n",
    "            text_clean.append(texts[lisS[h]:lisI[h+1]])\n",
    "\n",
    "    if separator != 'non':\n",
    "        description = str(text_clean).replace('>', '').replace(\n",
    "            ',', '').replace('\\'', '').replace('，', '')\n",
    "        description = description.split(separator)\n",
    "    else:\n",
    "        description = text_clean\n",
    "\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(dic: dict, nb:int):\n",
    "    \"\"\" Code allows to recup datas\n",
    "    Paramters: \n",
    "        dic = dictionnary having name and link airport\n",
    "    Outer:\n",
    "        dataAirline = dataFrame having scrap informations\n",
    "    \n",
    "    \"\"\"\n",
    "    name_col = ['Airline_Name', 'Airline_Type', 'Region_Operation', 'Aircraft_Type', 'Cabin_Class', 'Type_Of_Lounge',\n",
    "                'Type_Of_Traveller', 'Date_Visit', 'Date_Flown', 'Airport', 'Route', 'Category', 'Category_Detail',\n",
    "                'Cabin_Staff_Service', 'Lounge_Staff_Service', 'Bar_And_Beverages', 'Food_And_Beverages', 'Ground_Service', 'Catering', 'Cleanliness',\n",
    "                'Lounge_Comfort', 'Aisle_Space', 'Wifi_And_Connectivity', 'Inflight_Entertainment', 'Viewing_Tv_Screen', 'Power_Supply',\n",
    "                'Seat', 'Seat_type', 'Seat_Comfort', 'Seat_Legroom', 'Seat_Storage', 'Seat_Width', 'Seat_Recline', 'Washrooms',\n",
    "                'Value_For_Money', 'Overall_Customer_Rating', 'Overall_Service_Rating', 'Overall_Airline_Rating',\n",
    "                'Recommended', 'Departure_city', 'Arrival_city', 'Nb_bus_taken', 'Nb_train_taken',\n",
    "                'Nb_car_taken', 'Nb_plane_taken', 'Duration', 'Price_min', 'Price_max', 'Nb_sharing', 'Awards', 'Registration', 'Language',\n",
    "                'Queuing Times', 'Terminal_Seating', 'Terminal Signs', 'Airport_Shopping', 'Experience_At_Airport', 'Date_Review']\n",
    "\n",
    "    dataAirline = pd.DataFrame(columns=name_col)\n",
    "\n",
    "    for dic_key, dic_val in dic.items():\n",
    "        r = requests.get(dic_val)\n",
    "        page = r.text\n",
    "        soup = bs(page, 'html.parser')\n",
    "        nb_page = Nb_pages(soup)\n",
    "\n",
    "        for j in range(1, nb_page+1):\n",
    "            r = requests.get(dic_val + '/page/' + str(j) + '/')\n",
    "            page = r.text\n",
    "            soup = bs(page, 'html.parser')\n",
    "\n",
    "            Date_Review = dateReview(soup, nb)\n",
    "            \n",
    "            title = title_comm(soup, nb)\n",
    "            desc = description(soup, nb)\n",
    "            note = UserNot(soup, nb)\n",
    "            notGlo = NoteGlobal(soup, nb)\n",
    "            \n",
    "\n",
    "            airport = []\n",
    "            for i in range(0, len(desc)):\n",
    "                airport.append(dic_key)\n",
    "\n",
    "            df = pd.DataFrame(data=[title, desc, note, airport])\n",
    "            df = df.transpose()\n",
    "\n",
    "            Title = df[0]\n",
    "            Review = df[1]\n",
    "            Date_Visit, Terminal_Cleanliness, Food_Beverages, Wifi_Connectivity, Airport_Staff, Recommended, Type_Of_Traveller, Queuing_Times, Terminal_Seating, Terminal_Signs, Airport_Shopping, Experience_At_Airport = transformColInDic(\n",
    "                df[2])\n",
    "            Airport = df[3]\n",
    "\n",
    "            df_template = pd.DataFrame({'Date_Flown': Date_Visit, 'Cleanliness': Terminal_Cleanliness, 'Food_And_Beverages': Food_Beverages,\n",
    "                                        'Wifi_And_Connectivity': Wifi_Connectivity, 'Cabin_Staff_Service': Airport_Staff, 'Overall_Customer_Rating': notGlo,\n",
    "                                        'Recommended': Recommended, 'Title': Title, 'Review': Review, 'Airport': Airport, 'Type_Of_Traveller': Type_Of_Traveller,\n",
    "                                        'Queuing_Times': Queuing_Times, 'Terminal_Seating': Terminal_Seating, 'Terminal_Signs': Terminal_Signs,\n",
    "                                        'Airport_Shopping': Airport_Shopping, 'Experience_At_Airport': Experience_At_Airport, 'Date_Review': Date_Review})\n",
    "\n",
    "            dataAirline = pd.concat([dataAirline, df_template])\n",
    "\n",
    "    return dataAirline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserNot(soup: str, nb:int):\n",
    "    \"\"\" Function to retrieve users' notes using the two notes functions. \n",
    "        The first one retrieves the maximum notes for each user. \n",
    "        The second one retrieves the category 'NA' \n",
    "    \n",
    "    \"\"\"\n",
    "    list_not = notation2(soup, nb)\n",
    "    noteUser = []\n",
    "    value = []\n",
    "    list_total = [' 1', '2', '3', '4', '5']\n",
    "    for z in range(0, len(list_not)):\n",
    "        dico = {}\n",
    "        del list_not[z][0]\n",
    "        for i in range(0, len(list_not[z])-2):\n",
    "            if len(str(list_not[z][i]).replace(' ', '')) > 1:\n",
    "                if len(str(list_not[z][i+1]).replace(' ', '')) > 1:\n",
    "                    if list_not[z][i] not in value:\n",
    "                        dico[list_not[z][i]] = list_not[z][i+1]\n",
    "                        value.append(list_not[z][i+1])\n",
    "                else:\n",
    "                    j = i\n",
    "                    while str(list_not[z][j+1]) in list_total:\n",
    "                        dico[list_not[z][i]] = list_not[z][j+1]\n",
    "                        j = j + 1\n",
    "        noteUser.append(dico)\n",
    "\n",
    "    counter_user = 0\n",
    "    c_user_not_w_NA = 0\n",
    "    p = notation(soup, nb)\n",
    "    for k in noteUser:\n",
    "        value = []\n",
    "        t = 0\n",
    "        for key, val in k.items():\n",
    "            if val != 'N/A':\n",
    "                if val == '5':\n",
    "                    noteUser[counter_user][key] = p[c_user_not_w_NA][t]\n",
    "                    t = t + 1\n",
    "                    if t == len(p[c_user_not_w_NA]):\n",
    "                        c_user_not_w_NA = c_user_not_w_NA + 1\n",
    "\n",
    "        counter_user = counter_user + 1\n",
    "    return noteUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a date to standard format\n",
    "def format_date(date):\n",
    "    # Transform a string date into a standard format by trying each\n",
    "    # date format. If you want to add a format, add a try/except in the\n",
    "    # last except\n",
    "    # date : str : the date to transform\n",
    "    # return : m : timedata : format is YYYY-MM-DD HH:MM:SS\n",
    "    date_str = date\n",
    "    #\n",
    "    date_str = date_str.replace(\"st\",\"\").replace(\"th\",\"\")        .replace(\"nd\",\"\").replace(\"rd\",\"\").replace(\" Augu \",\" Aug \")\n",
    "    m = None\n",
    "    try:\n",
    "        m = datetime.strptime(date_str, \"%d %B %Y\")\n",
    "    except ValueError:\n",
    "        try:\n",
    "            m = datetime.strptime(date_str, \"%d %b %Y\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                m = datetime.strptime(date_str, \"%Y/%m/%d\")\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    m = datetime                        .strptime(date_str,\"%d/%m/%Y %H:%M:%S\")\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        m = datetime                            .strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    except ValueError:\n",
    "                        try :\n",
    "                            m = datetime.strptime(date_str,\n",
    "                                                       \"%d %m %Y\")\n",
    "                        except ValueError:\n",
    "                            # HERE ADD A FORMAT TO CHECK\n",
    "                            print(\"Format not recognised. \\nConsider \"\n",
    "                                  \"adding a date format \"\n",
    "                                  \"in the function \\\"format_date\\\".\")\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformColInDic(col_dic: dict):\n",
    "    # Prend en paramètre la colonne du DF contenant le dictionnaire des notes\n",
    "    Date_Visit = []\n",
    "    Terminal_Cleanliness = []\n",
    "    Food_Beverages = []\n",
    "    Wifi_Connectivity = []\n",
    "    Airport_Staff = []\n",
    "    Recommended = []\n",
    "    Type_Of_Traveller = []\n",
    "    Queuing_Times = []\n",
    "    Terminal_Seating = []\n",
    "    Airport_shopping = []\n",
    "    Terminal_Signs = []\n",
    "    Experience_At_Airport = []\n",
    "\n",
    "    for i in range(0, len(col_dic)):\n",
    "        if 'Date Visit' in (col_dic[i]).keys():\n",
    "            Date_Visit.append((col_dic[i]['Date Visit']))\n",
    "        else:\n",
    "            Date_Visit.append(' ')\n",
    "\n",
    "        if ' Terminal Cleanliness' in (col_dic[i]).keys():\n",
    "            Terminal_Cleanliness.append((col_dic[i][' Terminal Cleanliness']))\n",
    "        else:\n",
    "            Terminal_Cleanliness.append(' ')\n",
    "\n",
    "        if ' Food Beverages' in (col_dic[i]).keys():\n",
    "            Food_Beverages.append((col_dic[i][' Food Beverages']))\n",
    "        else:\n",
    "            Food_Beverages.append(' ')\n",
    "\n",
    "        if ' Wifi Connectivity' in (col_dic[i]).keys():\n",
    "            Wifi_Connectivity.append((col_dic[i][' Wifi Connectivity']))\n",
    "        else:\n",
    "            Wifi_Connectivity.append(' ')\n",
    "\n",
    "        if ' Airport Staff' in (col_dic[i]).keys():\n",
    "            Airport_Staff.append((col_dic[i][' Airport Staff']))\n",
    "        else:\n",
    "            Airport_Staff.append(' ')\n",
    "\n",
    "        if ' Recommended' in (col_dic[i]).keys():\n",
    "            Recommended.append((col_dic[i][' Recommended']))\n",
    "        else:\n",
    "            Recommended.append(' ')\n",
    "\n",
    "        if 'Type Of Traveller' in (col_dic[i]).keys():\n",
    "            Type_Of_Traveller.append((col_dic[i]['Type Of Traveller']))\n",
    "        else:\n",
    "            Type_Of_Traveller.append(' ')\n",
    "\n",
    "        if 'Queuing Times' in (col_dic[i]).keys():\n",
    "            Queuing_Times.append((col_dic[i]['Queuing Times']))\n",
    "        else:\n",
    "            Queuing_Times.append(' ')\n",
    "\n",
    "        if ' Terminal Seating' in (col_dic[i]).keys():\n",
    "            Terminal_Seating.append((col_dic[i][' Terminal Seating']))\n",
    "        else:\n",
    "            Terminal_Seating.append(' ')\n",
    "\n",
    "        if ' Airport Shopping' in (col_dic[i]).keys():\n",
    "            Airport_shopping.append((col_dic[i][' Airport Shopping']))\n",
    "        else:\n",
    "            Airport_shopping.append(' ')\n",
    "\n",
    "        if ' Terminal Signs' in (col_dic[i]).keys():\n",
    "            Terminal_Signs.append((col_dic[i][' Terminal Signs']))\n",
    "        else:\n",
    "            Terminal_Signs.append(' ')\n",
    "\n",
    "        if 'Experience At Airport' in (col_dic[i]).keys():\n",
    "            Experience_At_Airport.append((col_dic[i]['Experience At Airport']))\n",
    "        else:\n",
    "            Experience_At_Airport.append(' ')\n",
    "\n",
    "    return Date_Visit, Terminal_Cleanliness, Food_Beverages, Wifi_Connectivity, Airport_Staff, Recommended, Type_Of_Traveller, Queuing_Times, Terminal_Seating, Terminal_Signs, Airport_shopping, Experience_At_Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notation(soup: str, nb:int):\n",
    "    note = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "        \n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            tab_not = span.findAll('span', attrs={'class': 'star fill'})\n",
    "            notation_categ = re.findall(r'[0-9]', str(tab_not))\n",
    "            if len(notation_categ) > 0:\n",
    "                noteUser = []\n",
    "                len_not_categ = len(notation_categ)\n",
    "                for i in range(0, len_not_categ-1):\n",
    "                    if notation_categ[i] >= notation_categ[i+1]:\n",
    "                        noteUser.append(notation_categ[i])\n",
    "\n",
    "                noteUser.append(notation_categ[len_not_categ-1])\n",
    "                note.append(noteUser)\n",
    "    \n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_comm(soup: str, nb:int):\n",
    "    title = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            top = span.findAll('h2', attrs={'class': 'text_header'})\n",
    "            top = translate(recovTextBetweenTags(str(top), 'non'))\n",
    "            title.append(top[0][1:len(top[0])])\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateReview(soup: str, nb:int):\n",
    "    dateR = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            top = span.findAll('time', attrs={'itemprop': 'datePublished'})\n",
    "            dateR.append(recovTextBetweenTags(str(top), ','))\n",
    "\n",
    "    return dateR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoteGlobal(soup: str, nb: int):\n",
    "    notGlo = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            top = span.findAll('span', attrs={'itemprop': 'ratingValue'})\n",
    "            notGlo.append(recovTextBetweenTags(str(top), ','))\n",
    "\n",
    "    return notGlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description(soup: str, nb:int):\n",
    "    desc = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            top = span.findAll('div', attrs={'class': 'text_content'})\n",
    "            desc.append(translate(recovTextBetweenTags(str(top), ',')))\n",
    "\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notation2(soup: str, nb:int):\n",
    "    note = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        \n",
    "        dat = str(recovTextBetweenTags(str(span.findAll('time', attrs={\n",
    "                  'itemprop': 'datePublished'})), ',')).replace(\"['[\", '').replace(\"]']\", '')\n",
    "        dat = (format_date(dat))\n",
    "        if (dat) > (datetime.now() - timedelta(nb)):\n",
    "            \n",
    "            not_tot_tab = span.findAll('table', attrs={'class': 'review-ratings'})\n",
    "            not_tot_tab = (recovTextBetweenTags(str(not_tot_tab), ','))\n",
    "            note.append(str(str(not_tot_tab).replace(\n",
    "                '\\\\n', '').replace('\\\\', '')).split('  '))\n",
    "\n",
    "    Rating = []\n",
    "    for elem in note:\n",
    "        if len(elem) != 0:\n",
    "            Rating.append(elem)\n",
    "\n",
    "    return Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nb_pages(soup: str):\n",
    "    nb_page_total = soup.find('div', attrs={'class': 'pagination-total'})\n",
    "    if nb_page_total != None:\n",
    "        nb_page_total = str(nb_page_total)\n",
    "        nb_pages = int(nb_page_total[41:len(nb_page_total)-14])//10 + 1\n",
    "    else:\n",
    "        nb_pages = 1\n",
    "    return(nb_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching scrapping + json recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addJSON(file: str, df, creat: bool):\n",
    "\n",
    "    if creat is False :\n",
    "        with open(file) as train_file:\n",
    "            dict_train = json.load(train_file)\n",
    "        data = pd.read_json(dict_train, orient=\"records\")\n",
    "        df = pd.concat([data, df])\n",
    "    \n",
    "    js = df.to_json(orient='records').replace(\n",
    "        \"[\\\\\\\"[\", '').replace(\"]\\\\\\\"]\", '')\n",
    "    \n",
    "    with open(file, 'w', encoding='utf8') as outfile:\n",
    "        json.dump(js, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Aircraft_Type, Airline_Name, Airline_Type, Airport, Airport_Shopping, Aisle_Space, Arrival_city, Awards, Bar_And_Beverages, Cabin_Class, Cabin_Staff_Service, Category, Category_Detail, Catering, Cleanliness, Date_Flown, Date_Review, Date_Visit, Departure_city, Duration, Experience_At_Airport, Food_And_Beverages, Ground_Service, Inflight_Entertainment, Language, Lounge_Comfort, Lounge_Staff_Service, Nb_bus_taken, Nb_car_taken, Nb_plane_taken, Nb_sharing, Nb_train_taken, Overall_Airline_Rating, Overall_Customer_Rating, Overall_Service_Rating, Power_Supply, Price_max, Price_min, Queuing Times, Queuing_Times, Recommended, Region_Operation, Registration, Review, Route, Seat, Seat_Comfort, Seat_Legroom, Seat_Recline, Seat_Storage, Seat_Width, Seat_type, Terminal Signs, Terminal_Seating, Terminal_Signs, Title, Type_Of_Lounge, Type_Of_Traveller, Value_For_Money, Viewing_Tv_Screen, Washrooms, Wifi_And_Connectivity]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 62 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:59: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Starting the scrap function\n",
    "df = scrap(dice, 20000)\n",
    "# Transformation and Complete of Json\n",
    "addJSON(file, df, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
