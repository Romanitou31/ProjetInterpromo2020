{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "# import ssl\n",
    "import json\n",
    "# import ast\n",
    "# import os\n",
    "from urllib.request import Request, urlopen\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from langdetect import detect\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancement du scrapping + Enregistrement en json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = corp(dic)\n",
    "js = df.to_json(orient='records').replace(\"[\", '').replace(\"]\", '')\n",
    "with open('testWeibo.json', 'w', encoding='utf8') as outfile:\n",
    "    json.dump(js, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getAirportLinks(texte):\n",
    "    # Récupération des aéroports\n",
    "    airport = re.findall(\"\\\">(.*?)</a></li>\", str(texte))[0]\n",
    "    link = re.findall(\"href\\=(.*?)>\", str(texte))[0].replace(\"\\\"\", \"\")\n",
    "    return airport, link\n",
    "\n",
    "\n",
    "def createDictionnary():\n",
    "    # Création du dictionnaire contenant le nom de l'aéroport ainsi que son URL\n",
    "    dic = {}\n",
    "    racine = \"https://www.airlinequality.com\"\n",
    "    url_page = racine+\"/review-pages/a-z-airport-reviews/\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "    req = Request(url_page, headers=headers)\n",
    "    webpage = urlopen(req).read()\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "\n",
    "    r = soup.find_all('li')\n",
    "    liste_text = [str(val) for val in r if \"href=\\\"/airport-reviews/\" in str(val)\n",
    "                  and \"article\" not in str(val)]\n",
    "    for texte in liste_text:\n",
    "        airport, link = getAirportLinks(texte)\n",
    "        dic[airport.rstrip()] = racine+link\n",
    "\n",
    "    return dic\n",
    "\n",
    "\n",
    "dic = createDictionnary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupTexteEntreBalise(texte, separateur):\n",
    "    # Récupération du code entre deux balises\n",
    "    texte2 = []\n",
    "    lisI = []\n",
    "    lisS = []\n",
    "\n",
    "    for i in range(0, len(texte)):\n",
    "        if str(texte[i]) == \"<\":\n",
    "            lisI.append(i)\n",
    "        if texte[i] == '>':\n",
    "            lisS.append(i)\n",
    "\n",
    "    taille = len(lisI)\n",
    "    for h in range(0, taille):\n",
    "        if h < (taille-1):\n",
    "            texte2.append(texte[lisS[h]:lisI[h+1]])\n",
    "\n",
    "    if separateur != 'non':\n",
    "        description = str(texte2).replace('>', '').replace(\n",
    "            ',', '').replace('\\'', '').replace('，', '')\n",
    "        description = description.split(separateur)\n",
    "    else:\n",
    "        description = texte2\n",
    "\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corp(dic):\n",
    "    # \n",
    "    nom_col = ['Airline_Name', 'Airline_Type', 'Region_Operation', 'Aircraft_Type', 'Cabin_Class', 'Type_Of_Lounge',\n",
    "               'Type_Of_Traveller', 'Date_Visit', 'Date_Flown', 'Airport', 'Route', 'Category', 'Category_Detail',\n",
    "               'Cabin_Staff_Service', 'Lounge_Staff_Service', 'Bar_And_Beverages', 'Food_And_Beverages', 'Ground_Service', 'Catering', 'Cleanliness',\n",
    "               'Lounge_Comfort', 'Aisle_Space', 'Wifi_And_Connectivity', 'Inflight_Entertainment', 'Viewing_Tv_Screen', 'Power_Supply',\n",
    "               'Seat', 'Seat_type', 'Seat_Comfort', 'Seat_Legroom', 'Seat_Storage', 'Seat_Width', 'Seat_Recline', 'Washrooms',\n",
    "               'Value_For_Money', 'Overall_Customer_Rating', 'Overall_Service_Rating', 'Overall_Airline_Rating',\n",
    "               'Recommended', 'Departure_city', 'Arrival_city', 'Nb_bus_taken', 'Nb_train_taken',\n",
    "               'Nb_car_taken', 'Nb_plane_taken', 'Duration', 'Price_min', 'Price_max', 'Nb_sharing', 'Awards', 'Registration', 'Language',\n",
    "               'Queuing Times', 'Terminal_Seating', 'Terminal Signs', 'Airport_Shopping', 'Experience_At_Airport', 'Date_Review']\n",
    "\n",
    "    dataAirline = pd.DataFrame(columns=nom_col)\n",
    "\n",
    "    for azerty, qsdfg in dic.items():\n",
    "        r = requests.get(qsdfg)\n",
    "        page = r.text\n",
    "        soup = bs(page, 'html.parser')\n",
    "        nb_page = Nb_pages(soup)\n",
    "\n",
    "        for j in range(1, nb_page+1):\n",
    "            r = requests.get(qsdfg+'/page/'+str(j)+'/')\n",
    "            page = r.text\n",
    "            soup = bs(page, 'html.parser')\n",
    "\n",
    "            title = titre(soup)\n",
    "            desc = description(soup)\n",
    "            note = UserNot(soup)\n",
    "            notGlo = NoteGlobale(soup)\n",
    "            Date_Review = dateReview(soup)\n",
    "\n",
    "            airport = []\n",
    "            for i in range(0, len(desc)):\n",
    "                airport.append(azerty)\n",
    "\n",
    "            df = pd.DataFrame(data=[title, desc, note, airport])\n",
    "            df = df.transpose()\n",
    "\n",
    "            Title = df[0]\n",
    "            Review = df[1]\n",
    "            Date_Visit, Terminal_Cleanliness, Food_Beverages, Wifi_Connectivity, Airport_Staff, Recommended, Type_Of_Traveller, Queuing_Times, Terminal_Seating, Terminal_Signs, Airport_Shopping, Experience_At_Airport = dic_col(\n",
    "                df[2])\n",
    "            Airport = df[3]\n",
    "\n",
    "            o = pd.DataFrame({'Date_Flown': Date_Visit, 'Cleanliness': Terminal_Cleanliness, 'Food_And_Beverages': Food_Beverages,\n",
    "                              'Wifi_And_Connectivity': Wifi_Connectivity, 'Cabin_Staff_Service': Airport_Staff, 'Overall_Customer_Rating': notGlo,\n",
    "                              'Recommended': Recommended, 'Title': Title, 'Review': Review, 'Airport': Airport, 'Type_Of_Traveller': Type_Of_Traveller,\n",
    "                              'Queuing_Times': Queuing_Times, 'Terminal_Seating': Terminal_Seating, 'Terminal_Signs': Terminal_Signs,\n",
    "                              'Airport_Shopping': Airport_Shopping, 'Experience_At_Airport': Experience_At_Airport, 'Date_Review': Date_Review})\n",
    "\n",
    "            dataAirline = pd.concat([dataAirline, o])\n",
    "\n",
    "    return dataAirline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserNot(soup):\n",
    "    liste = notation2(soup)\n",
    "    noteUser = []\n",
    "    value = []\n",
    "    liste1 = [' 1', '2', '3', '4', '5']\n",
    "    for z in range(0, len(liste)):\n",
    "        dico = {}\n",
    "        del liste[z][0]\n",
    "        for i in range(0, len(liste[z])-2):\n",
    "            if len(str(liste[z][i]).replace(' ', '')) > 1:\n",
    "                if len(str(liste[z][i+1]).replace(' ', '')) > 1:\n",
    "                    if liste[z][i] not in value:\n",
    "                        dico[liste[z][i]] = liste[z][i+1]\n",
    "                        value.append(liste[z][i+1])\n",
    "                else:\n",
    "                    j = i\n",
    "                    while str(liste[z][j+1]) in liste1:\n",
    "                        dico[liste[z][i]] = liste[z][j+1]\n",
    "                        j = j+1\n",
    "        noteUser.append(dico)\n",
    "\n",
    "    y = 0\n",
    "    f = 0\n",
    "    p = notation(soup)\n",
    "    for k in noteUser:\n",
    "        value = []\n",
    "        t = 0\n",
    "        for cle, valeur in k.items():\n",
    "            if valeur != 'N/A':\n",
    "                if valeur == '5':\n",
    "                    noteUser[y][cle] = p[f][t]\n",
    "                    t = t + 1\n",
    "                    if t == len(p[f]):\n",
    "                        f = f + 1\n",
    "\n",
    "        y = y+1\n",
    "    return noteUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_col(o):\n",
    "    # Prend en paramètre la colonne du DF contenant le dictionnaire des notes\n",
    "    Date_Visit = []\n",
    "    Terminal_Cleanliness = []\n",
    "    Food_Beverages = []\n",
    "    Wifi_Connectivity = []\n",
    "    Airport_Staff = []\n",
    "    Recommended = []\n",
    "    Type_Of_Traveller = []\n",
    "    Queuing_Times = []\n",
    "    Terminal_Seating = []\n",
    "    Airport_shopping = []\n",
    "    Terminal_Signs = []\n",
    "    Experience_At_Airport = []\n",
    "\n",
    "    for i in range(0, len(o)):\n",
    "        if 'Date Visit' in (o[i]).keys():\n",
    "            Date_Visit.append((o[i]['Date Visit']))\n",
    "        else:\n",
    "            Date_Visit.append(' ')\n",
    "\n",
    "        if ' Terminal Cleanliness' in (o[i]).keys():\n",
    "            Terminal_Cleanliness.append((o[i][' Terminal Cleanliness']))\n",
    "        else:\n",
    "            Terminal_Cleanliness.append(' ')\n",
    "\n",
    "        if ' Food Beverages' in (o[i]).keys():\n",
    "            Food_Beverages.append((o[i][' Food Beverages']))\n",
    "        else:\n",
    "            Food_Beverages.append(' ')\n",
    "\n",
    "        if ' Wifi Connectivity' in (o[i]).keys():\n",
    "            Wifi_Connectivity.append((o[i][' Wifi Connectivity']))\n",
    "        else:\n",
    "            Wifi_Connectivity.append(' ')\n",
    "\n",
    "        if ' Airport Staff' in (o[i]).keys():\n",
    "            Airport_Staff.append((o[i][' Airport Staff']))\n",
    "        else:\n",
    "            Airport_Staff.append(' ')\n",
    "\n",
    "        if ' Recommended' in (o[i]).keys():\n",
    "            Recommended.append((o[i][' Recommended']))\n",
    "        else:\n",
    "            Recommended.append(' ')\n",
    "\n",
    "        if 'Type Of Traveller' in (o[i]).keys():\n",
    "            Type_Of_Traveller.append((o[i]['Type Of Traveller']))\n",
    "        else:\n",
    "            Type_Of_Traveller.append(' ')\n",
    "\n",
    "        if 'Queuing Times' in (o[i]).keys():\n",
    "            Queuing_Times.append((o[i]['Queuing Times']))\n",
    "        else:\n",
    "            Queuing_Times.append(' ')\n",
    "\n",
    "        if ' Terminal Seating' in (o[i]).keys():\n",
    "            Terminal_Seating.append((o[i][' Terminal Seating']))\n",
    "        else:\n",
    "            Terminal_Seating.append(' ')\n",
    "\n",
    "        if ' Airport Shopping' in (o[i]).keys():\n",
    "            Airport_shopping.append((o[i][' Airport Shopping']))\n",
    "        else:\n",
    "            Airport_shopping.append(' ')\n",
    "\n",
    "        if ' Terminal Signs' in (o[i]).keys():\n",
    "            Terminal_Signs.append((o[i][' Terminal Signs']))\n",
    "        else:\n",
    "            Terminal_Signs.append(' ')\n",
    "\n",
    "        if 'Experience At Airport' in (o[i]).keys():\n",
    "            Experience_At_Airport.append((o[i]['Experience At Airport']))\n",
    "        else:\n",
    "            Experience_At_Airport.append(' ')\n",
    "\n",
    "    return Date_Visit, Terminal_Cleanliness, Food_Beverages, Wifi_Connectivity, Airport_Staff, Recommended, Type_Of_Traveller, Queuing_Times, Terminal_Seating, Terminal_Signs, Airport_shopping, Experience_At_Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notation(soup):\n",
    "    note = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        toto = span.findAll('span', attrs={'class': 'star fill'})\n",
    "        top = re.findall(r'[0-9]', str(toto))\n",
    "        if len(top) > 0:\n",
    "            noteUser = []\n",
    "            taille = len(top)\n",
    "            for i in range(0, taille-1):\n",
    "                if top[i] >= top[i+1]:\n",
    "                    noteUser.append(top[i])\n",
    "            noteUser.append(top[taille-1])\n",
    "            note.append(noteUser)\n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titre(soup):\n",
    "    title = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        top = span.findAll('h2', attrs={'class': 'text_header'})\n",
    "        top = recupTexteEntreBalise(str(top), 'non')\n",
    "        title.append(top[0][1:len(top[0])])\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateReview(soup):\n",
    "    dateR = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        top = span.findAll('time', attrs={'itemprop': 'datePublished'})\n",
    "\n",
    "        dateR.append(recupTexteEntreBalise(str(top), ','))\n",
    "\n",
    "    return dateR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoteGlobale(soup):\n",
    "    notGlo = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        top = span.findAll('span', attrs={'itemprop': 'ratingValue'})\n",
    "        notGlo.append(recupTexteEntreBalise(str(top), ','))\n",
    "\n",
    "    return notGlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description(soup):\n",
    "    desc = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        top = span.findAll('div', attrs={'class': 'text_content'})\n",
    "        desc.append(recupTexteEntreBalise(str(top), ','))\n",
    "\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notation2(soup):\n",
    "    note = []\n",
    "    for span in soup.findAll('article', attrs={'itemprop': 'review'}):\n",
    "        toto = span.findAll('table', attrs={'class': 'review-ratings'})\n",
    "        toto = (recupTexteEntreBalise(str(toto), ','))\n",
    "        note.append(str(str(toto).replace(\n",
    "            '\\\\n', '').replace('\\\\', '')).split('  '))\n",
    "\n",
    "    Rating = []\n",
    "    for elem in note:\n",
    "        if len(elem) != 0:\n",
    "            Rating.append(elem)\n",
    "\n",
    "    return Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nb_pages(soup):\n",
    "    toto = soup.find('div', attrs={'class': 'pagination-total'})\n",
    "    if toto != None:\n",
    "        toto = str(toto)\n",
    "        nb_pages = int(toto[41:len(toto)-14])//10 + 1\n",
    "    else:\n",
    "        nb_pages = 1\n",
    "    return(nb_pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
