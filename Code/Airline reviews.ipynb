{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good = Date_Flown, Cleanliness, Food_And_Beverages, Wifi_And_Connectivity, Cabin_Staff_Service,Recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad = Queuing Times,Terminal Seating, Terminal Signs, Airport shopping,Experience At Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import json\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "from urllib.request import Request, urlopen\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recupTexteEntreBalise(texte, separateur):\n",
    "    \n",
    "    texte2 = []\n",
    "    lisI = []\n",
    "    lisS = []\n",
    "    \n",
    "    for i in range(0,len(texte)):\n",
    "        if str(texte[i]) == \"<\":\n",
    "            lisI.append(i)\n",
    "        if texte[i] == '>':\n",
    "            lisS.append(i)   \n",
    "\n",
    "    taille = len(lisI)\n",
    "    for h in range(0,taille):\n",
    "        if h < (taille-1):\n",
    "            texte2.append(texte[lisS[h]:lisI[h+1]])\n",
    "    \n",
    "    if separateur != 'non':\n",
    "        description = str(texte2).replace('>','').replace(',','').replace('\\'','').replace('，','')\n",
    "        description = description.split(separateur)\n",
    "    else:\n",
    "        description = texte2\n",
    "    \n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = {}\n",
    "dice['LivingStone']='https://www.airlinequality.com/airport-reviews/livingstone-airport'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corp(dic):\n",
    "    \n",
    "    nom_col = ['Airline_Name','Airline_Type','Region_Operation','Aircraft_Type','Cabin_Class','Type_Of_Lounge',\n",
    "                   'Type_Of_Traveller','Date_Visit','Date_Flown','Airport','Route','Category','Category_Detail',\n",
    "                   'Cabin_Staff_Service','Lounge_Staff_Service','Bar_And_Beverages','Food_And_Beverages','Ground_Service','Catering','Cleanliness',\n",
    "                  'Lounge_Comfort','Aisle_Space','Wifi_And_Connectivity','Inflight_Entertainment','Viewing_Tv_Screen','Power_Supply',\n",
    "                  'Seat','Seat_type','Seat_Comfort','Seat_Legroom','Seat_Storage','Seat_Width','Seat_Recline','Washrooms',\n",
    "                   'Value_For_Money','Overall_Customer_Rating','Overall_Service_Rating','Overall_Airline_Rating',\n",
    "                  'Recommended','Departure_city','Arrival_city','Nb_bus_taken','Nb_train_taken',\n",
    "                   'Nb_car_taken','Nb_plane_taken','Duration','Price_min','Price_max','Nb_sharing','Awards','Registration','Language']\n",
    "    \n",
    "    dataAirline = pd.DataFrame(columns = nom_col)\n",
    "    \n",
    "    for azerty, qsdfg in dic.items():\n",
    "        r = requests.get(qsdfg)\n",
    "        page = r.text\n",
    "        soup = bs(page,'html.parser')\n",
    "        nb_page = Nb_pages(soup)\n",
    "\n",
    "        for j in range (1,nb_page+1):\n",
    "            r = requests.get(qsdfg+'/page/'+str(j)+'/')\n",
    "            page = r.text\n",
    "            soup = bs(page,'html.parser')\n",
    "\n",
    "            title = titre(soup)\n",
    "            desc = description(soup)\n",
    "            note = UserNot(soup)\n",
    "\n",
    "            airport=[]\n",
    "            for i in range(0,len(desc)):\n",
    "                airport.append(azerty)\n",
    "\n",
    "            df = pd.DataFrame(data=[title, desc, note, airport])\n",
    "            df = df.transpose()\n",
    " \n",
    "            Title = df[0]\n",
    "            Review = df[1]\n",
    "            Date_Visit, Terminal_Cleanliness, Food_Beverages, Wifi_Connectivity, Airport_Staff, Recommended, Type_Of_Traveller = dic_col(df[2])\n",
    "            Airport = df[3]\n",
    "\n",
    "            o = pd.DataFrame({'Date_Flown': Date_Visit, 'Cleanliness': Terminal_Cleanliness, 'Food_And_Beverages': Food_Beverages,\n",
    "                              'Wifi_And_Connectivity' : Wifi_Connectivity, 'Cabin_Staff_Service': Airport_Staff,\n",
    "                             'Recommended':Recommended, 'Title': Title, 'Review': Review, 'Airport': Airport, 'Type_Of_Traveller' : Type_Of_Traveller })\n",
    "\n",
    "            dataAirline = pd.concat([dataAirline, o])\n",
    "\n",
    "\n",
    "    return dataAirline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getAirportLinks(texte):\n",
    "    airport=re.findall(\"\\\">(.*?)</a></li>\",str(texte))[0]\n",
    "    link=re.findall(\"href\\=(.*?)>\",str(texte))[0].replace(\"\\\"\",\"\")\n",
    "    return airport,link\n",
    "def createDictionnary():\n",
    "    dic={}\n",
    "    racine=\"https://www.airlinequality.com\"\n",
    "    url_page=racine+\"/review-pages/a-z-airport-reviews/\"\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "    req = Request(url_page,headers=headers)\n",
    "    webpage = urlopen(req).read()\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    \n",
    "    r=soup.find_all('li')\n",
    "    liste_text=[str(val) for val in r if \"href=\\\"/airport-reviews/\" in str(val) and \"article\" not in str(val) ]\n",
    "    for texte in liste_text:\n",
    "        airport,link=getAirportLinks(texte)\n",
    "        dic[airport.rstrip()]=racine+link\n",
    "    \n",
    "    return dic\n",
    "dic = createDictionnary()\n",
    "\n",
    "# del dic['Livingstone']\n",
    "# del dic['Mombasa']\n",
    "# del dic['Toronto City Centre']\n",
    "# del dic['Victoria Falls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserNot(soup):\n",
    "    liste = notation2(soup)\n",
    "    noteUser = []\n",
    "    value = []\n",
    "    liste1 = [' 1','2','3','4','5']\n",
    "    for z in range(0,len(liste)):\n",
    "        dico = {}\n",
    "        del liste[z][0]\n",
    "        for i in range(0,len(liste[z])-2):\n",
    "            if len(str(liste[z][i]).replace(' ',''))>1:\n",
    "                if len(str(liste[z][i+1]).replace(' ',''))>1: \n",
    "                    if liste[z][i] not in value:\n",
    "                        dico[liste[z][i]]=liste[z][i+1]\n",
    "                        value.append(liste[z][i+1])\n",
    "                else:\n",
    "                    j=i\n",
    "                    while str(liste[z][j+1]) in liste1:\n",
    "                        dico[liste[z][i]]=liste[z][j+1]\n",
    "                        j=j+1\n",
    "        noteUser.append(dico)\n",
    "\n",
    "    y=0\n",
    "    f=0\n",
    "    p = notation(soup)\n",
    "    for k in noteUser:    \n",
    "        value = []\n",
    "        t=0\n",
    "        for cle,valeur in k.items():\n",
    "            if valeur != 'N/A':\n",
    "                if valeur == '5':\n",
    "                    noteUser[y][cle] = p[f][t]\n",
    "                    t = t + 1\n",
    "                    if t == len(p[f]):\n",
    "                        f = f + 1\n",
    "                \n",
    "        y=y+1\n",
    "    return noteUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_col(o):\n",
    "# Prend en paramètre la colonne du DF contenant le dictionnaire des notes\n",
    "    Date_Visit = []\n",
    "    Terminal_Cleanliness = [] \n",
    "    Food_Beverages = [] \n",
    "    Wifi_Connectivity = []\n",
    "    Airport_Staff = []\n",
    "    Recommended = []\n",
    "    Type_Of_Traveller = []\n",
    "\n",
    "    for i in range(0,len(o)):\n",
    "        if 'Date Visit' in (o[i]).keys():\n",
    "            Date_Visit.append((o[i]['Date Visit']))\n",
    "        else:\n",
    "            Date_Visit.append(' ')\n",
    "\n",
    "        if ' Terminal Cleanliness' in (o[i]).keys():\n",
    "            Terminal_Cleanliness.append((o[i][' Terminal Cleanliness']))\n",
    "        else:\n",
    "            Terminal_Cleanliness.append(' ')\n",
    "\n",
    "        if ' Food Beverages' in (o[i]).keys():\n",
    "            Food_Beverages.append((o[i][' Food Beverages']))\n",
    "        else:\n",
    "            Food_Beverages.append(' ')\n",
    "\n",
    "        if ' Wifi Connectivity' in (o[i]).keys():\n",
    "            Wifi_Connectivity.append((o[i][' Wifi Connectivity']))\n",
    "        else:\n",
    "            Wifi_Connectivity.append(' ')\n",
    "\n",
    "        if ' Airport Staff' in (o[i]).keys():\n",
    "            Airport_Staff.append((o[i][' Airport Staff']))\n",
    "        else:\n",
    "            Airport_Staff.append(' ')\n",
    "\n",
    "        if ' Recommended' in (o[i]).keys():\n",
    "            Recommended.append((o[i][' Recommended']))\n",
    "        else:\n",
    "            Recommended.append(' ')\n",
    "            \n",
    "        if 'Type Of Traveller' in (o[i]).keys():\n",
    "            Type_Of_Traveller.append((o[i]['Type Of Traveller']))\n",
    "        else:\n",
    "            Type_Of_Traveller.append(' ')\n",
    "            \n",
    "    return  Date_Visit, Terminal_Cleanliness, Food_Beverages, Wifi_Connectivity, Airport_Staff, Recommended, Type_Of_Traveller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notation(soup):\n",
    "    note = []\n",
    "    for span in soup.findAll('article',attrs={'itemprop':'review'}):\n",
    "        toto = span.findAll('span',attrs={'class':'star fill'})\n",
    "        top = re.findall(r'[0-9]',str(toto))\n",
    "        if len(top)>0:\n",
    "            noteUser = []\n",
    "            taille = len(top)\n",
    "            for i in range(0,taille-1):\n",
    "                if top[i] >= top[i+1]:\n",
    "                    noteUser.append(top[i])\n",
    "            noteUser.append(top[taille-1])\n",
    "            note.append(noteUser)\n",
    "    return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titre(soup):\n",
    "    title = []\n",
    "    for span in soup.findAll('article',attrs={'itemprop':'review'}):\n",
    "        top = span.findAll('h2',attrs={'class':'text_header'})\n",
    "        top = recupTexteEntreBalise(str(top),'non')\n",
    "        title.append(top[0][1:len(top[0])])\n",
    "        \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description(soup):\n",
    "    desc = []\n",
    "    for span in soup.findAll('article',attrs={'itemprop':'review'}):\n",
    "        top = span.findAll('div',attrs={'class':'text_content'})\n",
    "        desc.append(recupTexteEntreBalise(str(top),','))\n",
    "        \n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notation2(soup):\n",
    "    note = []\n",
    "    for span in soup.findAll('article',attrs={'itemprop':'review'}):\n",
    "        toto = span.findAll('table',attrs={'class':'review-ratings'})\n",
    "        toto = (recupTexteEntreBalise(str(toto),','))\n",
    "        note.append(str(str(toto).replace('\\\\n','').replace('\\\\','')).split('  '))\n",
    "    \n",
    "    Rating = []\n",
    "    for elem in note:\n",
    "        if len(elem) != 0:\n",
    "            Rating.append(elem)\n",
    "\n",
    "    return Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nb_pages(soup):\n",
    "    toto = soup.find('div',attrs={'class':'pagination-total'})\n",
    "    if toto != None :\n",
    "        toto = str(toto)\n",
    "        nb_pages = int(toto[41:len(toto)-14])//10 +1\n",
    "    else : \n",
    "        nb_pages = 1\n",
    "    return(nb_pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
