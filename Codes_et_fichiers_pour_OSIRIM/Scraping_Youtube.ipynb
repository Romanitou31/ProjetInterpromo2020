{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created on 09/01/2020\n",
    "# Group1\n",
    "# @authors: benjamin anton\n",
    "\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta,date\n",
    "import time\n",
    "import requests\n",
    "from urllib.request import Request, urlopen\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path phantomJS\n",
    "path = './phantomjs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate (text) : \n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            text : character string \n",
    "       out : \n",
    "            text : text translated in english\n",
    "    \"\"\"\n",
    "    try :\n",
    "        new = str(TextBlob(text).translate(to='en'))\n",
    "        return new\n",
    "    except :\n",
    "        return text\n",
    "    \n",
    "def replace_all(text, dic):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            text : character string\n",
    "            dic : dictionary which contains the changes to be made\n",
    "       out : \n",
    "            text : text with all changes made \n",
    "    \"\"\"\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create equation research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Airline_Companies = [\n",
    "    \"American Airlines\",\n",
    "    \"Air Canada\",\n",
    "    \"Air France\",\n",
    "    \"Air Algerie\",\n",
    "    \"Air India\",\n",
    "    \"Aerolineas Argentinas\",\n",
    "    \"Royal Air Maroc\",\n",
    "    \"Finnair\",\n",
    "    \"Alitalia \",\n",
    "    \" Nouvelair\",\n",
    "    \"Air China\",\n",
    "    \"Cathay Pacific\",\n",
    "    \"Delta Airlines\",\n",
    "    \"Aer Lingus\",\n",
    "    \"Emirates\",\n",
    "    \"Ethiopian Airlines\",\n",
    "    \"Icelandair\",\n",
    "    \"Hawaiian Airlines\",\n",
    "    \"Iberia\",\n",
    "    \"Meridiana\",\n",
    "    \"Japan Airlines\",\n",
    "    \"KLM\",\n",
    "    \"Air Malta\",\n",
    "    \"Lan Airlines\",\n",
    "    \"Luxair\",\n",
    "    \"LIAT\",\n",
    "    \"LOT Polish Airlines\",\n",
    "    \"Air Madagascar\",\n",
    "    \"Air Mauritius\",\n",
    "    \"Austrian Airlines\",\n",
    "    \"Qatar Airways\",\n",
    "    \"South African Airways\",\n",
    "    \"SAS Scandinavian Airlines\",\n",
    "    \"Brussels Airlines\",\n",
    "    \"Singapore Airlines\",\n",
    "    \"Corsair\",\n",
    "    \"Aeroflot\",\n",
    "    \"Thai Airways\",\n",
    "    \"Turkish Airlines\",\n",
    "    \"TAP Portugal\",\n",
    "    \"Air Transat\",\n",
    "    \"Tunisair\",\n",
    "    \"Air Caraibes\",\n",
    "    \"United Airlines\",\n",
    "    \"Air Austral\",\n",
    "    \"Air Europa\",\n",
    "    \"Easyjet\",\n",
    "    \"Vietnam Airlines\",\n",
    "    \"Virgin Atlantic\",\n",
    "    \"Air Corsica\",\n",
    "    \"Condor\",\n",
    "    \"Flybe\",\n",
    "    \"Aegean Airlines\",\n",
    "    \"Air Tahiti Nui\",\n",
    "    \"Aigle Azur\",\n",
    "    \"HOP!\",\n",
    "    \"Jet Airways\",\n",
    "    \"Etihad Airways\",\n",
    "    \"Etihad Airways\",\n",
    "    \"Oman Air\",\n",
    "    \"XL Airways\",\n",
    "    \"Ryanair LTD\",\n",
    "    \"Vueling \",\n",
    "    \"Norwegian\",\n",
    "    \"Transavia France\",\n",
    "    \"Germanwings\",\n",
    "    \"TUI Fly Belgium\",\n",
    "    \"Air Arabia\",\n",
    "    \"WOW air\",\n",
    "    \"Wizz Air\",\n",
    "    \"Air Asia\",\n",
    "    \"Volotea\",\n",
    "    \"southwest airlines\"\n",
    "]\n",
    "Airline_Companies = [compagnies.replace(\n",
    "    ' ', '+') for compagnies in Airline_Companies]\n",
    "\n",
    "\n",
    "Boeing_Models = [\n",
    "    \"Boeing 717\",\n",
    "    \"Boeing 727\",\n",
    "    \"Boeing 737-200\",\n",
    "    \"Boeing 737-300\",\n",
    "    \"Boeing 737-400\",\n",
    "    \"Boeing 737-500\",\n",
    "    \"Boeing 737-600\",\n",
    "    \"Boeing 737-700\",\n",
    "    \"Boeing 737-700ER\",\n",
    "    \"Boeing 737-800\",\n",
    "    \"Boeing 737-900\",\n",
    "    \"Boeing 737-900ER\",\n",
    "    \"Boeing 737 MAX 7\",\n",
    "    \"Boeing 737 MAX 8\",\n",
    "    \"Boeing 737 MAX 9\",\n",
    "    \"Boeing 737 MAX 10\",\n",
    "    \"Boeing 747-200\",\n",
    "    \"Boeing 747-400\",\n",
    "    \"Boeing 757-200\",\n",
    "    \"Boeing 757-300\",\n",
    "    \"Boeing 767-200\",\n",
    "    \"Boeing 767-300\",\n",
    "    \"Boeing 767-300ER\",\n",
    "    \"Boeing 767-400ER\",\n",
    "    \"Boeing 777 Triple Seven\",\n",
    "    \"Boeing 787 DreamLiner\"\n",
    "]\n",
    "Boeing_Models = [models.replace(' ', '+') for models in Boeing_Models]\n",
    "\n",
    "\n",
    "Airbus_Models = [\n",
    "    \"A300\",\n",
    "    \"A300-600ST\",\n",
    "    \"A318\",\n",
    "    \"A319\",\n",
    "    \"A320-100\",\n",
    "    \"A320-200\",\n",
    "    \"A320neo\",\n",
    "    \"A321-100\",\n",
    "    \"A321-200\",\n",
    "    \"A330-200\",\n",
    "    \"A330-300\",\n",
    "    \"A330-200F\",\n",
    "    \"A330-500\",\n",
    "    \"A340-200\",\n",
    "    \"A340-300\",\n",
    "    \"A340-500\",\n",
    "    \"A340-600\",\n",
    "    \"A350-900\",\n",
    "    \"A350-1000\",\n",
    "    \"A380-800\",\n",
    "    \"A220-300\"\n",
    "]\n",
    "\n",
    "key_words = [\n",
    "    \"trip\",\n",
    "    \"fly\",\n",
    "    \"plane\",\n",
    "    \"airplane\",\n",
    "    \"flight\"\n",
    "]\n",
    "\n",
    "\n",
    "equations = []\n",
    "for comp in Airline_Companies:\n",
    "    for mod in Airbus_Models:\n",
    "        equations.append(comp+\"+\"+mod)\n",
    "    for mod in Boeing_Models:\n",
    "        equations.append(comp+\"+\"+mod)\n",
    "\n",
    "# quibbling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChangeDate(chain):\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            chain : character string in format \"ex :il ya 300 ans\"    \n",
    "       out : \n",
    "            expression.group(0) : character string in format  \"ex 300 ans\"          \n",
    "    \"\"\"\n",
    "    expression = re.search(\"[0-9]*\\s[a-zA-Z]*$\", chain)\n",
    "    return(expression.group(0))\n",
    "\n",
    "\n",
    "def Simplification(chain):\n",
    "    # transform M into 1000000,  and K into 1000\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            chain : character string in format \"ex : 300K\"    \n",
    "       out : \n",
    "            int(float(expression) * coef) : integer  \"ex 300 000\"          \n",
    "    \"\"\"\n",
    "    chain = chain.replace(',', '.')\n",
    "    if '.' in chain:\n",
    "        expression = (re.search(\"\\d+\\.\\d+\", chain)).group(0)\n",
    "    else:\n",
    "        expression = (re.search(\"\\d+\", chain)).group(0)\n",
    "    coef = 1\n",
    "    if 'k' in chain:\n",
    "        coef = 1000\n",
    "    if 'M' in chain:\n",
    "        coef = 1000000\n",
    "    return(int(float(expression) * coef))\n",
    "\n",
    "\n",
    "def DateCalculation(chain):\n",
    "    # calculate the exact date of publication of the video\n",
    "    \"\"\"Documentation    \n",
    "       Parameters :\n",
    "            chain : character string in format \"ex : il ya 300 ans\"    \n",
    "       out : \n",
    "            chain : date           \n",
    "    \"\"\"\n",
    "\n",
    "    chain = (chain).replace('il y a ', '')\n",
    "    day_nb=0\n",
    "    if 'hier' in chain:\n",
    "        day_nb = 1\n",
    "    else:\n",
    "        day_nb = int((re.search(\"\\d+\", chain)).group(0))\n",
    "    if 'mois' in chain:\n",
    "        day_nb = day_nb * 30\n",
    "    if 'an' in chain:\n",
    "        day_nb = day_nb * 365\n",
    "    if 'semaine' in chain:\n",
    "        day_nb = day_nb * 7\n",
    "\n",
    "    now = date.today()\n",
    "    return str(now - timedelta(days=(day_nb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the url list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def URLlist(Research_Equations):\n",
    "    # returns the list of videos of the search equation passed in parameter\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            Research_Equations : search equation we want to launch    \n",
    "       out : \n",
    "            list_Videos : search equation videos list        \n",
    "    \"\"\"\n",
    "    root_URL = \"https://www.youtube.com/results?search_query=\"\n",
    "    #ResearchEquations = \"airbus+A380\"\n",
    "\n",
    "    r = requests.get(root_URL + Research_Equations + '&sp=EgIIAw%253D%253D')\n",
    "    page = r.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    list_Videos = []\n",
    "    for videos in soup.findAll('a', attrs={'class': 'yt-uix-tile-link'}) : \n",
    "        list_Videos.append('https://www.youtube.com'+videos['href'])\n",
    "    \n",
    "    return list_Videos\n",
    "\n",
    "\n",
    "list_Videos = []\n",
    "for Equation in equations:\n",
    "    list_Videos += URLlist(Equation)\n",
    "list_Videos = np.unique(list_Videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function that creates our filled Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCodeHTML(URL_list, fig):\n",
    "    \"\"\"Documentation    \n",
    "\n",
    "       Parameters:\n",
    "            URL_list : list of url   \n",
    "            fig : url index processed\n",
    "       out :  \n",
    "            BeautifulSoup(web_page, 'html.parser') : return html code of the url page      \n",
    "    \"\"\"\n",
    "    url = URL_list[fig]\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    web_page = urlopen(req).read()\n",
    "    time.sleep(2)\n",
    "    return BeautifulSoup(web_page, 'html.parser')\n",
    "\n",
    "\n",
    "def CreateJs(comment, nb_com, soup, comment_date):\n",
    "    # create the json file with all the information on the page\n",
    "    \"\"\"Documentation    \n",
    "\n",
    "       Parameters:\n",
    "            comment : character string of the comment\n",
    "            nb_com : number of comment\n",
    "            comment_date : comment date\n",
    "\n",
    "    \"\"\"\n",
    "    name_col = ['Airline_Name',\n",
    "                'Airline_Type',\n",
    "                'Region_Operation',\n",
    "                'Aircraft_Type',\n",
    "                'Cabin_Class',\n",
    "                'Type_Of_Lounge',\n",
    "                'Type_Of_Traveller',\n",
    "                'Date_Visit',\n",
    "                'Date_Flown',\n",
    "                'Airport',\n",
    "                'Route',\n",
    "                'Category',\n",
    "                'Category_Detail',\n",
    "                'Cabin_Staff_Service',\n",
    "                'Lounge_Staff_Service',\n",
    "                'Bar_And_Beverages',\n",
    "                'Food_And_Beverages',\n",
    "                'Ground_Service',\n",
    "                'Catering', 'Cleanliness',\n",
    "                'Lounge_Comfort',\n",
    "                'Aisle_Space',\n",
    "                'Wifi_And_Connectivity',\n",
    "                'Inflight_Entertainment',\n",
    "                'Viewing_Tv_Screen',\n",
    "                'Power_Supply',\n",
    "                'Seat',\n",
    "                'Seat_type',\n",
    "                'Seat_Comfort',\n",
    "                'Seat_Legroom',\n",
    "                'Seat_Storage',\n",
    "                'Seat_Width',\n",
    "                'Seat_Recline',\n",
    "                'Washrooms',\n",
    "                'Value_For_Money',\n",
    "                'Overall_Customer_Rating',\n",
    "                'Overall_Service_Rating',\n",
    "                'Overall_Airline_Rating',\n",
    "                'Recommended',\n",
    "                'Departure_city',\n",
    "                'Arrival_city',\n",
    "                'Nb_bus_taken',\n",
    "                'Nb_train_taken',\n",
    "                'Nb_car_taken',\n",
    "                'Nb_plane_taken',\n",
    "                'Duration',\n",
    "                'Price_min',\n",
    "                'Price_max',\n",
    "                'Nb_sharing',\n",
    "                'Awards',\n",
    "                'Registration',\n",
    "                'Language']\n",
    "\n",
    "    soup = soup\n",
    "    video_details = {}\n",
    "\n",
    "# Fill data\n",
    "\n",
    "    video_details['Data_Source'] = 'Youtube'\n",
    "\n",
    "    for i in range(39):\n",
    "        video_details[name_col[i]] = ' '\n",
    "\n",
    "    video_details['Date_Review'] = DateCalculation(comment_date)\n",
    "    video_details['Review'] = translate(comment)\n",
    "\n",
    "    for i in range(39, 48):\n",
    "        video_details[name_col[i]] = ' '\n",
    "\n",
    "# get the title of the video\n",
    "    if soup.find('span', attrs={'class': 'watch-title'}) == None :\n",
    "         video_details['Title'] = ' '\n",
    "    else :\n",
    "        video_details['Title'] = soup.find(\n",
    "            'span', attrs={'class': 'watch-title'}).text.strip()\n",
    "\n",
    "# get the name of the chain\n",
    "    if soup.findAll('script', attrs={'type': 'application/ld+json'}) == None :\n",
    "        video_details['Author'] = ''\n",
    "    else :\n",
    "        for script in soup.findAll('script', attrs={'type': 'application/ld+json'}):\n",
    "            channelDescription = json.loads(script.text.strip())\n",
    "            video_details['Author'] = channelDescription['itemListElement'][0]['item']['name']\n",
    "\n",
    "# get description\n",
    "    if soup.find('p', attrs={'id': \"eow-description\"}) == None :\n",
    "        video_details['Description'] = ''\n",
    "    else :\n",
    "        video_details['Description'] = soup.find(\n",
    "            'p', attrs={'id': \"eow-description\"}).text.strip()\n",
    "\n",
    "# get the date of publication\n",
    "    dic = {'.':'','avr':'apr','janv':'jan','mars':'mar','mai':'may','juin':'jun','févr':'feb','juil':'jul','déc':'dec','août':'aug','sept':'sep','aoÃ»t':'aug','dÃ©c':'dec'}\n",
    "    var_date_of_public = soup.find('strong',attrs={'class': \"watch-time-text\"}).text.strip().replace('.','')\n",
    "    var_date_of_public = replace_all(var_date_of_public,dic)\n",
    "    var_date_not_None = re.search(\"[0-9][0-9]* [a-zA-Z]* [0-9]*\", var_date_of_public)\n",
    "    if var_date_not_None == None :\n",
    "        video_details['Date_publication'] = ''\n",
    "    else :\n",
    "        video_details['Date_publication'] = str(datetime.strptime(var_date_not_None.group(0), '%d %b %Y')). replace('00:00:00', '')\n",
    "\n",
    "# get the number of views\n",
    "    if soup.find('div', attrs={'class': 'watch-view-count'}) == None :\n",
    "        video_details['View_Count'] = ''\n",
    "    else :\n",
    "        video_details['View_Count'] = (soup.find(\n",
    "            'div', attrs={'class': 'watch-view-count'}).text.strip()).replace('vues', '')\n",
    "\n",
    "# get a likes button\n",
    "    if soup.findAll('', attrs={'class': \"yt-uix-button yt-uix-button-size-default yt-uix-button-opacity yt-uix-button-has-icon no-icon-markup like-button-renderer-like-button like-button-renderer-like-button-unclicked yt-uix-clickcard-target yt-uix-tooltip\"}) == None:\n",
    "        video_details['Likes'] = ''\n",
    "    else :\n",
    "        for span in soup.findAll('', attrs={'class': \"yt-uix-button yt-uix-button-size-default yt-uix-button-opacity yt-uix-button-has-icon no-icon-markup like-button-renderer-like-button like-button-renderer-like-button-unclicked yt-uix-clickcard-target yt-uix-tooltip\"}):\n",
    "            video_details['Likes'] = span.find(\n",
    "                'span', attrs={'class': 'yt-uix-button-content'}).text.strip()\n",
    "\n",
    "# get a dislikes button\n",
    "    if soup.findAll('button', attrs={'class': \"yt-uix-button yt-uix-button-size-default yt-uix-button-opacity yt-uix-button-has-icon no-icon-markup like-button-renderer-dislike-button like-button-renderer-dislike-button-unclicked yt-uix-clickcard-target yt-uix-tooltip\"}) == None:\n",
    "        video_details['Dislikes'] = ''\n",
    "    else :\n",
    "        for button in soup.findAll('button', attrs={'class': \"yt-uix-button yt-uix-button-size-default yt-uix-button-opacity yt-uix-button-has-icon no-icon-markup like-button-renderer-dislike-button like-button-renderer-dislike-button-unclicked yt-uix-clickcard-target yt-uix-tooltip\"}):\n",
    "            video_details['Dislikes'] = button.find(\n",
    "                'span', attrs={'class': 'yt-uix-button-content'}).text.strip()\n",
    "\n",
    "# get subscriber number\n",
    "    if (soup.find('span', attrs={'class': 'yt-subscription-button-subscriber-count-branded-horizontal yt-subscriber-count'}) == None):\n",
    "        video_details[\"Nb_subscribers\"] = 0\n",
    "    else:\n",
    "        video_details[\"Nb_subscribers\"] = Simplification(soup.find('span', attrs={\n",
    "                                                         'class': 'yt-subscription-button-subscriber-count-branded-horizontal yt-subscriber-count'}).text.strip())\n",
    "\n",
    "    video_details['Nb_comments'] = (nb_com).replace('commentaires', '')\n",
    "\n",
    "    video_details[name_col[48]] = ' '\n",
    "# get hashtags\n",
    "    hashtags = []\n",
    "\n",
    "    for span in soup.findAll('span', attrs={'class': 'standalone-collection-badge-renderer-text'}):\n",
    "        for a in span.findAll('a', attrs={'class': 'yt-uix-sessionlink'}):\n",
    "            hashtags.append(a.text.strip())\n",
    "    video_details['hashtags'] = hashtags\n",
    "\n",
    "    for i in range(49, 51):\n",
    "        video_details[name_col[i]] = ' '\n",
    "\n",
    "    video_details['Language'] = 'unknown'\n",
    "\n",
    "    if re.search(\"([a-z]).*\", str(comment).lower()) : \n",
    "        try :\n",
    "            video_details['Language'] = detect(comment)\n",
    "        except :\n",
    "            video_details['Language'] = 'unknown'\n",
    "\n",
    "        \n",
    "    with open('../RESULTATS_JSON/data_Youtube.json', 'a', encoding='utf8') as outfile:\n",
    "        json.dump(video_details, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def scroll(url, nb_scroll):\n",
    "    #scroll down in the page\n",
    "    \"\"\"Documentation    \n",
    "       Parameters:\n",
    "            url : page url \n",
    "            nb_scroll : number of times you scroll  \n",
    "       out : \n",
    "            BeautifulSoup(driver.page_source, 'html.parser') : the new page after scroll\n",
    "            \n",
    "\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    driver =  webdriver.PhantomJS(path)\n",
    "    driver.get(url)\n",
    "    Y = 0\n",
    "    for _ in range(nb_scroll):\n",
    "        time.sleep(4)\n",
    "        driver.execute_script(\"window.scrollTo(\"+str(Y)+\",\"+str(Y+800)+\")\")\n",
    "        Y += 1200\n",
    "\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vidéo numéro :  0  fini\n",
      "vidéo numéro :  1  fini\n",
      "vidéo numéro :  2  fini\n",
      "vidéo numéro :  3  fini\n",
      "vidéo numéro :  4  fini\n",
      "vidéo numéro :  5  fini\n",
      "vidéo numéro :  6  fini\n",
      "vidéo numéro :  7  fini\n",
      "vidéo numéro :  8  fini\n",
      "vidéo numéro :  9  fini\n",
      "vidéo numéro :  10  fini\n",
      "vidéo numéro :  11  fini\n",
      "vidéo numéro :  12  fini\n",
      "vidéo numéro :  13  fini\n",
      "vidéo numéro :  14  fini\n",
      "vidéo numéro :  15  fini\n",
      "vidéo numéro :  16  fini\n",
      "vidéo numéro :  17  fini\n",
      "vidéo numéro :  18  fini\n",
      "vidéo numéro :  19  fini\n",
      "extraction complete\n",
      "CPU times: user 37.4 s, sys: 577 ms, total: 38 s\n",
      "Wall time: 7min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a new json\n",
    "\n",
    "with open('../RESULTATS_JSON/data_Youtube.json', 'w', encoding='utf8') as outfile:\n",
    "    json\n",
    "\n",
    "for URL_unique in range(len(list_Videos)):\n",
    "    soup1 = scroll(list_Videos[URL_unique], 4)\n",
    "    \n",
    "    SoupCréeJS = GetCodeHTML(list_Videos, URL_unique)\n",
    "# date comment\n",
    "    date1 = []\n",
    "    for span1 in soup1.findAll('span', attrs={'class': \"comment-renderer-time\"}):\n",
    "        a = (span1.find('a', attrs={'class': \"yt-uix-sessionlink spf-link\"}).text.strip())\n",
    "        date1.append(a)\n",
    "    date_Track = 0\n",
    "    for span in soup1.findAll('div', attrs={'class': 'comment-renderer-text-content'}):\n",
    "        if span.text.strip() != '':\n",
    "            comment = span.text.strip()\n",
    "            nb_com = re.search(\"[0-9][0-9]*\", (soup1.find('h2', \n",
    "                                                         attrs={'class': 'comment-section-header-renderer'}).text.strip()).replace('\\xa0',' ').replace('\\u202f','')).group(0)\n",
    "            CreateJs(comment, nb_com, SoupCréeJS, date1[date_Track])\n",
    "            date_Track += 1\n",
    "    print('vidéo numéro : ', URL_unique, ' fini')\n",
    "print('extraction complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
